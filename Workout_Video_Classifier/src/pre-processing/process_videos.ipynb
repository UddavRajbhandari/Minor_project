{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define landmark groups and exercise weights\n",
    "LANDMARK_GROUPS = {\n",
    "    'arms': [11,12,13,14,15,16],  # shoulder to wrist landmarks\n",
    "    'legs': [23,24,25,26,27,28],  # hip to ankle landmarks\n",
    "    'core': [23,24,11,12],  # hips and shoulders\n",
    "    'upper_body': [11,12,13,14,15,16,0,1,2,3,4,5,6,7,8,9,10]  # above hips\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXERCISE_WEIGHTS = {\n",
    "    #'bench press': {'arms': 0.5, 'core': 0.3, 'legs': 0.2},\n",
    "    'barbell biceps curl': {'arms': 0.7, 'core': 0.2, 'legs': 0.1},\n",
    "    #'chest fly machine': {'arms': 0.5, 'core': 0.4, 'legs': 0.1},\n",
    "    #'deadlift': {'legs': 0.4, 'core': 0.4, 'arms': 0.2},\n",
    "    #'decline bench press': {'arms': 0.5, 'core': 0.3, 'legs': 0.2},\n",
    "    'hammer curl': {'arms': 0.7, 'core': 0.2, 'legs': 0.1},\n",
    "    #'hip thrust': {'legs': 0.5, 'core': 0.4, 'arms': 0.1},\n",
    "    #'incline bench press': {'arms': 0.5, 'core': 0.3, 'legs': 0.2},\n",
    "    'lat pulldown': {'arms': 0.5, 'core': 0.4, 'legs': 0.1},\n",
    "    'lateral raise': {'arms': 0.6, 'core': 0.3, 'legs': 0.1},\n",
    "    #'leg extension': {'legs': 0.7, 'core': 0.2, 'arms': 0.1},\n",
    "    #'leg raises': {'core': 0.6, 'legs': 0.3, 'arms': 0.1},\n",
    "    #'plank': {'core': 0.7, 'arms': 0.2, 'legs': 0.1},\n",
    "    'pull Up': {'arms': 0.5, 'core': 0.4, 'legs': 0.1},\n",
    "    'push-up': {'arms': 0.5, 'core': 0.4, 'legs': 0.1},\n",
    "    #'romanian deadlift': {'legs': 0.5, 'core': 0.3, 'arms': 0.2},\n",
    "    #'russian twist': {'core': 0.6, 'arms': 0.2, 'legs': 0.2},\n",
    "    'shoulder press': {'arms': 0.6, 'core': 0.3, 'legs': 0.1},\n",
    "    #'squat': {'legs': 0.6, 'core': 0.3, 'arms': 0.1},\n",
    "    #'t bar row': {'arms': 0.4, 'core': 0.4, 'legs': 0.2},\n",
    "   # 'tricep dips': {'arms': 0.6, 'core': 0.3, 'legs': 0.1},\n",
    "    #'tricep Pushdown': {'arms': 0.7, 'core': 0.2, 'legs': 0.1}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkoutDataPreparator:\n",
    "    def __init__(self, base_folder, target_classes=None, max_frames=45):\n",
    "        self.base_folder = base_folder\n",
    "        self.target_classes = target_classes\n",
    "        self.max_frames = max_frames\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(\n",
    "            static_image_mode=False,\n",
    "            model_complexity=2, #model 2 insted of 1\n",
    "            min_detection_confidence=0.5\n",
    "        )\n",
    "\n",
    "    def calculate_weighted_confidence(self, landmarks, exercise_type):\n",
    "        weights = EXERCISE_WEIGHTS[exercise_type]\n",
    "        weighted_scores = np.zeros(33)  # One score per landmark\n",
    "        \n",
    "        # Calculate weighted confidence for each landmark\n",
    "        for group, weight in weights.items():\n",
    "            group_landmarks = LANDMARK_GROUPS[group]\n",
    "            for lm_idx in group_landmarks:\n",
    "                weighted_scores[lm_idx] = landmarks[lm_idx].visibility * weight\n",
    "                \n",
    "        return weighted_scores\n",
    "    \n",
    "    def normalize_poses(self, poses):\n",
    "        normalized_poses = []\n",
    "        for pose in poses:\n",
    "            landmarks = pose.reshape(-1, 4)\n",
    "            left_hip = landmarks[23][:3]\n",
    "            right_hip = landmarks[24][:3]\n",
    "            hip_center = (left_hip + right_hip) / 2\n",
    "            normalized_landmarks = landmarks.copy()\n",
    "            normalized_landmarks[:, :3] -= hip_center\n",
    "            normalized_poses.append(normalized_landmarks.flatten())\n",
    "        return np.array(normalized_poses)\n",
    "\n",
    "    def calculate_velocity(self, poses):\n",
    "        velocity = np.zeros_like(poses)\n",
    "        velocity[1:] = poses[1:] - poses[:-1]\n",
    "        return velocity\n",
    "\n",
    "    def extract_poses_from_video(self, video_path, exercise_type):\n",
    "        poses = []\n",
    "        confidence_scores = []\n",
    "        weighted_confidences = []\n",
    "        \n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                raise ValueError(f\"Could not open video file: {video_path}\")\n",
    "                \n",
    "            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            frame_indices = np.linspace(0, total_frames-1, self.max_frames, dtype=int)\n",
    "            \n",
    "            for frame_idx in frame_indices:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                    \n",
    "                frame = cv2.resize(frame, (640, 480))\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = self.pose.process(frame_rgb)\n",
    "\n",
    "                if results.pose_landmarks:\n",
    "                    # Original confidence calculation\n",
    "                    confidence = np.mean([lm.visibility for lm in results.pose_landmarks.landmark])\n",
    "                    \n",
    "                    # Calculate weighted confidence scores\n",
    "                    weighted_conf = self.calculate_weighted_confidence(\n",
    "                        results.pose_landmarks.landmark, \n",
    "                        exercise_type\n",
    "                    )\n",
    "                    \n",
    "                    if confidence > 0.4:\n",
    "                        landmarks = np.array([[lm.x, lm.y, lm.z, lm.visibility] \n",
    "                                           for lm in results.pose_landmarks.landmark])\n",
    "                        poses.append(landmarks.flatten())\n",
    "                        confidence_scores.append(confidence)\n",
    "                        weighted_confidences.append(weighted_conf)\n",
    "                    else:\n",
    "                        poses.append(np.zeros(33 * 4))\n",
    "                        confidence_scores.append(0.0)\n",
    "                        weighted_confidences.append(np.zeros(33))\n",
    "                else:\n",
    "                    poses.append(np.zeros(33 * 4))\n",
    "                    confidence_scores.append(0.0)\n",
    "                    weighted_confidences.append(np.zeros(33))\n",
    "            \n",
    "            cap.release()\n",
    "            \n",
    "            if len(poses) < self.max_frames:\n",
    "                last_pose = poses[-1] if poses else np.zeros(33 * 4)\n",
    "                last_weighted_conf = weighted_confidences[-1] if weighted_confidences else np.zeros(33)\n",
    "                poses.extend([last_pose] * (self.max_frames - len(poses)))\n",
    "                confidence_scores.extend([0.0] * (self.max_frames - len(confidence_scores)))\n",
    "                weighted_confidences.extend([last_weighted_conf] * (self.max_frames - len(weighted_confidences)))\n",
    "            \n",
    "            poses = np.array(poses)\n",
    "            normalized_poses = self.normalize_poses(poses)\n",
    "            velocity_features = self.calculate_velocity(normalized_poses)\n",
    "            weighted_conf_features = np.array(weighted_confidences)\n",
    "            \n",
    "            # Concatenate all features\n",
    "            final_poses = np.concatenate([normalized_poses, velocity_features, weighted_conf_features], axis=1)\n",
    "            \n",
    "            return final_poses, np.array(confidence_scores)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {str(e)}\")\n",
    "            return np.zeros((self.max_frames, 33 * 4 * 2 + 33)), np.zeros(self.max_frames)  # Updated shape\n",
    "\n",
    "    def mirror_augment(self, poses):\n",
    "        mirrored = poses.copy()\n",
    "        # Split into components (now including weighted confidence)\n",
    "        orig_poses = mirrored[:, :33*4]\n",
    "        velocity = mirrored[:, 33*4:33*8]\n",
    "        weighted_conf = mirrored[:, 33*8:]  # Last 33 features are weighted confidence\n",
    "        \n",
    "        landmarks = orig_poses.reshape(-1, 33, 4)\n",
    "        landmarks[:, :, 0] = 1 - landmarks[:, :, 0]  # Mirror x coordinates\n",
    "        \n",
    "        pairs = [(1,2), (3,4), (5,6), (7,8), (9,10), (11,12), (13,14), (15,16),\n",
    "                (23,24), (25,26), (27,28), (29,30), (31,32)]\n",
    "        \n",
    "        for pair in pairs:\n",
    "            landmarks[:, [pair[0], pair[1]]] = landmarks[:, [pair[1], pair[0]]]\n",
    "            # Mirror weighted confidences for paired landmarks\n",
    "            weighted_conf[:, [pair[0], pair[1]]] = weighted_conf[:, [pair[1], pair[0]]]\n",
    "        \n",
    "        vel_landmarks = velocity.reshape(-1, 33, 4)\n",
    "        vel_landmarks[:, :, 0] *= -1  # Mirror x velocities\n",
    "        for pair in pairs:\n",
    "            vel_landmarks[:, [pair[0], pair[1]]] = vel_landmarks[:, [pair[1], pair[0]]]\n",
    "            \n",
    "        mirrored_poses = landmarks.reshape(poses.shape[0], -1)\n",
    "        mirrored_velocity = vel_landmarks.reshape(poses.shape[0], -1)\n",
    "        \n",
    "        # Concatenate all components back together\n",
    "        return np.concatenate([mirrored_poses, mirrored_velocity, weighted_conf], axis=1)\n",
    "\n",
    "    def prepare_and_save_dataset(self, save_dir):\n",
    "        X, y = [], []\n",
    "        confidences = []\n",
    "        class_mapping = {}\n",
    "        skipped_videos = defaultdict(int)\n",
    "        \n",
    "        print(\"Preparing dataset...\")\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        for root, _, files in os.walk(self.base_folder):\n",
    "            folder_name = os.path.basename(root)\n",
    "            \n",
    "            if self.target_classes and folder_name not in self.target_classes:\n",
    "                continue\n",
    "                \n",
    "            if folder_name not in class_mapping:\n",
    "                class_mapping[folder_name] = len(class_mapping)\n",
    "            \n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.mp4', '.mov', '.avi')):\n",
    "                    video_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        pose_sequence, conf_scores = self.extract_poses_from_video(video_path, folder_name)\n",
    "                        \n",
    "                        if np.mean(conf_scores) > 0.2:\n",
    "                            X.append(pose_sequence)\n",
    "                            y.append(class_mapping[folder_name])\n",
    "                            confidences.append(np.mean(conf_scores))\n",
    "                            \n",
    "                            mirrored_sequence = self.mirror_augment(pose_sequence)\n",
    "                            X.append(mirrored_sequence)\n",
    "                            y.append(class_mapping[folder_name])\n",
    "                            confidences.append(np.mean(conf_scores))\n",
    "                        else:\n",
    "                            skipped_videos[folder_name] += 1\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing {file}: {str(e)}\")\n",
    "                        skipped_videos[folder_name] += 1\n",
    "        \n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        confidences = np.array(confidences)\n",
    "        \n",
    "        print(\"\\nDataset Statistics:\")\n",
    "        for class_name, class_idx in class_mapping.items():\n",
    "            total = sum(1 for label in y if label == class_idx)\n",
    "            print(f\"{class_name}: {total} samples\")\n",
    "        \n",
    "        print(\"\\nFinal X shape:\", X.shape)  # Added to verify shape\n",
    "        \n",
    "        np.save(os.path.join(save_dir, 'X.npy'), X)\n",
    "        np.save(os.path.join(save_dir, 'y.npy'), y)\n",
    "        np.save(os.path.join(save_dir, 'confidences.npy'), confidences)\n",
    "        np.save(os.path.join(save_dir, 'class_mapping.npy'), class_mapping)\n",
    "        \n",
    "        print(f\"\\nData saved to {save_dir}\")\n",
    "        return X, y, class_mapping, confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "\n",
      "Dataset Statistics:\n",
      "barbell biceps curl: 118 samples\n",
      "hammer curl: 38 samples\n",
      "lat pulldown: 92 samples\n",
      "lateral raise: 74 samples\n",
      "pull Up: 52 samples\n",
      "push-up: 112 samples\n",
      "shoulder press: 34 samples\n",
      "\n",
      "Final X shape: (520, 45, 297)\n",
      "\n",
      "Data saved to D:\\minor\\workout_processed_data_landmark\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_folder = r\"D:\\minor\\Upper_body dataset\"\n",
    "    target_classes = [ 'barbell biceps curl', \n",
    "                      'hammer curl', \n",
    "                     'lat pulldown', 'lateral raise', \n",
    "                     'pull Up', 'push-up',  'shoulder press',\n",
    "                     ]\n",
    "    save_dir = r\"D:\\minor\\workout_processed_data_landmark\"\n",
    "    \n",
    "    preparator = WorkoutDataPreparator(\n",
    "        base_folder=base_folder,\n",
    "        target_classes=target_classes,\n",
    "        max_frames=45\n",
    "    )\n",
    "    \n",
    "    X, y, class_mapping, confidences = preparator.prepare_and_save_dataset(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
