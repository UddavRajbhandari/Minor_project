{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Dropout, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkoutModelTrainer:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.class_mapping = None\n",
    "        self.confidences = None\n",
    "        self.model = None\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        print(\"Loading preprocessed data...\")\n",
    "        self.X = np.load(os.path.join(self.data_dir, 'X.npy'))\n",
    "        self.y = np.load(os.path.join(self.data_dir, 'y.npy'))\n",
    "        self.confidences = np.load(os.path.join(self.data_dir, 'confidences.npy'))\n",
    "        self.class_mapping = np.load(os.path.join(self.data_dir, 'class_mapping.npy'),\n",
    "                                   allow_pickle=True).item()\n",
    "        \n",
    "        print(f\"Loaded {len(self.X)} samples\")\n",
    "        print(f\"Input shape: {self.X.shape}\")  # Added to verify shape\n",
    "        print(\"\\nClass distribution:\")\n",
    "        for class_name, class_idx in self.class_mapping.items():\n",
    "            total = sum(1 for label in self.y if label == class_idx)\n",
    "            print(f\"{class_name}: {total} samples\")\n",
    "            \n",
    "    def build_model(self, input_shape, num_classes):\n",
    "        model = tf.keras.Sequential([\n",
    "            Conv1D(64, kernel_size=3, padding='same', activation='relu',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.01)), \n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(0.35),\n",
    "        \n",
    "            LSTM(128, return_sequences=True,\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.35),\n",
    "        \n",
    "            LSTM(64, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.35),\n",
    "        \n",
    "            Dense(64, activation='relu',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "        \n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Print model summary with new input shape\n",
    "        print(\"\\nModel Summary:\")\n",
    "        model.build(input_shape=(None,) + input_shape)\n",
    "        model.summary()\n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def get_callbacks(self):\n",
    "        class OverfittingDetectionCallback(tf.keras.callbacks.Callback):\n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                if logs.get('accuracy') - logs.get('val_accuracy') > 0.2:\n",
    "                    print(\"\\nWarning: Possible overfitting detected!\")\n",
    "    \n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=12,\n",
    "                restore_best_weights=True,\n",
    "                min_delta=0.001\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            ),\n",
    "            OverfittingDetectionCallback()\n",
    "        ]\n",
    "    \n",
    "    def calculate_class_weights(self, y_train):\n",
    "        class_weights = {}\n",
    "        unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "        max_count = np.max(class_counts)\n",
    "        for cls, count in zip(unique_classes, class_counts):\n",
    "            class_weights[cls] = max_count / count\n",
    "        return class_weights\n",
    "\n",
    "    def save_training_plots(self, fold_histories, save_path):\n",
    "        plot_dir = os.path.join(save_path, 'training_plots')\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        for fold, history in enumerate(fold_histories):\n",
    "            plt.plot(history['accuracy'], label=f'Train (Fold {fold+1})', alpha=0.3)\n",
    "            plt.plot(history['val_accuracy'], label=f'Val (Fold {fold+1})', alpha=0.3)\n",
    "        \n",
    "        plt.title('Model Accuracy Across Folds (Enhanced CNN-LSTM with Weighted Features)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        for fold, history in enumerate(fold_histories):\n",
    "            plt.plot(history['loss'], label=f'Train (Fold {fold+1})', alpha=0.3)\n",
    "            plt.plot(history['val_loss'], label=f'Val (Fold {fold+1})', alpha=0.3)\n",
    "        \n",
    "        plt.title('Model Loss Across Folds (Enhanced CNN-LSTM with Weighted Features)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_dir, 'combined_training_curves.png'), \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def save_fold_results(self, fold_results, fold_histories, save_path):\n",
    "        results_dir = os.path.join(save_path, 'fold_results')\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        all_metrics = []\n",
    "        for fold, (result, history) in enumerate(zip(fold_results, fold_histories)):\n",
    "            fold_dir = os.path.join(results_dir, f'fold_{fold+1}')\n",
    "            os.makedirs(fold_dir, exist_ok=True)\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history['accuracy'], label='Training')\n",
    "            plt.plot(history['val_accuracy'], label='Validation')\n",
    "            plt.title(f'Model Accuracy - Fold {fold+1} (Enhanced CNN-LSTM with Weighted Features)')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history['loss'], label='Training')\n",
    "            plt.plot(history['val_loss'], label='Validation')\n",
    "            plt.title(f'Model Loss - Fold {fold+1} (Enhanced CNN-LSTM with Weighted Features)')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(fold_dir, 'training_curves.png'), \n",
    "                       bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            plt.figure(figsize=(15, 12))\n",
    "            sns.heatmap(result['confusion_matrix'], \n",
    "                       annot=True, \n",
    "                       fmt='d', \n",
    "                       cmap='Blues',\n",
    "                       xticklabels=list(self.class_mapping.keys()),\n",
    "                       yticklabels=list(self.class_mapping.keys()))\n",
    "            plt.title(f'Confusion Matrix - Fold {fold+1} (Enhanced CNN-LSTM with Weighted Features)')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(fold_dir, 'confusion_matrix.png'), \n",
    "                       bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            report_df = pd.DataFrame(result['classification_report']).transpose()\n",
    "            report_df.to_csv(os.path.join(fold_dir, 'classification_report.csv'))\n",
    "            \n",
    "            report_txt = classification_report(\n",
    "                result['y_true'],\n",
    "                result['y_pred'],\n",
    "                target_names=list(self.class_mapping.keys()),\n",
    "                digits=2,\n",
    "                zero_division=0\n",
    "            )\n",
    "            with open(os.path.join(fold_dir, 'classification_report.txt'), 'w') as f:\n",
    "                f.write(report_txt)\n",
    "            \n",
    "            all_metrics.append({\n",
    "                'fold': fold + 1,\n",
    "                'val_accuracy': result['val_accuracy'],\n",
    "                'val_loss': result['val_loss'],\n",
    "                'best_epoch': len(history['loss'])\n",
    "            })\n",
    "        \n",
    "        metrics_df = pd.DataFrame(all_metrics)\n",
    "        metrics_df.to_csv(os.path.join(results_dir, 'all_fold_metrics.csv'), index=False)\n",
    "        \n",
    "        avg_metrics = {\n",
    "            'mean_val_accuracy': metrics_df['val_accuracy'].mean(),\n",
    "            'std_val_accuracy': metrics_df['val_accuracy'].std(),\n",
    "            'mean_val_loss': metrics_df['val_loss'].mean(),\n",
    "            'std_val_loss': metrics_df['val_loss'].std(),\n",
    "            'mean_epochs': metrics_df['best_epoch'].mean()\n",
    "        }\n",
    "        \n",
    "        pd.DataFrame([avg_metrics]).to_csv(\n",
    "            os.path.join(results_dir, 'average_metrics.csv'), index=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\nK-Fold Cross Validation Results (Enhanced CNN-LSTM with Weighted Features):\")\n",
    "        print(f\"Mean Validation Accuracy: {avg_metrics['mean_val_accuracy']:.4f} ± {avg_metrics['std_val_accuracy']:.4f}\")\n",
    "        print(f\"Mean Validation Loss: {avg_metrics['mean_val_loss']:.4f} ± {avg_metrics['std_val_loss']:.4f}\")\n",
    "        print(f\"Average Epochs per Fold: {avg_metrics['mean_epochs']:.1f}\")\n",
    "\n",
    "    def train_with_kfold(self, epochs=100, batch_size=16, n_splits=5, save_path=None):\n",
    "        if save_path is None:\n",
    "            save_path = \"k_fold_CNN_LSTM_landmark\"\n",
    "    \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nStarting {n_splits}-fold stratified cross validation with Enhanced CNN-LSTM architecture...\")\n",
    "        print(f\"Input shape per sample: {self.X.shape[1:]}\")\n",
    "        \n",
    "        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        fold_results = []\n",
    "        fold_histories = []\n",
    "    \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(self.X, self.y)):\n",
    "            print(f\"\\nTraining Fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "            X_train, X_val = self.X[train_idx], self.X[val_idx]\n",
    "            y_train, y_val = self.y[train_idx], self.y[val_idx]\n",
    "            \n",
    "            print(\"\\nClass distribution in validation set:\")\n",
    "            unique, counts = np.unique(y_val, return_counts=True)\n",
    "            for class_idx, count in zip(unique, counts):\n",
    "                class_name = list(self.class_mapping.keys())[list(self.class_mapping.values()).index(class_idx)]\n",
    "                print(f\"{class_name}: {count} samples\")\n",
    "        \n",
    "            input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "            model = self.build_model(input_shape, len(self.class_mapping))\n",
    "        \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=self.get_callbacks(),\n",
    "                class_weight=self.calculate_class_weights(y_train),\n",
    "                verbose=1\n",
    "            )\n",
    "        \n",
    "            val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "            fold_results.append({\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'val_loss': val_loss,\n",
    "                'confusion_matrix': confusion_matrix(y_val, y_pred_classes),\n",
    "                'classification_report': classification_report(\n",
    "                    y_val, y_pred_classes,\n",
    "                    target_names=list(self.class_mapping.keys()),\n",
    "                    output_dict=True,\n",
    "                    zero_division=0\n",
    "                ),\n",
    "                'y_true': y_val,\n",
    "                'y_pred': y_pred_classes\n",
    "            })\n",
    "\n",
    "            fold_histories.append(history.history)\n",
    "            model.save(os.path.join(save_path, f'model_fold_{fold+1}.keras'))\n",
    "    \n",
    "        self.save_training_plots(fold_histories, save_path)\n",
    "        self.save_fold_results(fold_results, fold_histories, save_path)\n",
    "    \n",
    "        return fold_results, fold_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Loaded 520 samples\n",
      "Input shape: (520, 45, 297)\n",
      "\n",
      "Class distribution:\n",
      "barbell biceps curl: 118 samples\n",
      "hammer curl: 38 samples\n",
      "lat pulldown: 92 samples\n",
      "lateral raise: 74 samples\n",
      "pull Up: 52 samples\n",
      "push-up: 112 samples\n",
      "shoulder press: 34 samples\n",
      "\n",
      "Starting 5-fold stratified cross validation with Enhanced CNN-LSTM architecture...\n",
      "Input shape per sample: (45, 297)\n",
      "\n",
      "Training Fold 1/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 24 samples\n",
      "hammer curl: 8 samples\n",
      "lat pulldown: 18 samples\n",
      "lateral raise: 15 samples\n",
      "pull Up: 10 samples\n",
      "push-up: 23 samples\n",
      "shoulder press: 6 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 45, 64)            256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 22, 64)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 22, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 8s 85ms/step - loss: 8.7183 - accuracy: 0.1899 - val_loss: 6.4138 - val_accuracy: 0.2115 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 7.6524 - accuracy: 0.3462 - val_loss: 6.3348 - val_accuracy: 0.2885 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 7.0924 - accuracy: 0.4375 - val_loss: 6.2176 - val_accuracy: 0.3173 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 7.0047 - accuracy: 0.4495 - val_loss: 6.1142 - val_accuracy: 0.3077 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.9052 - accuracy: 0.4207 - val_loss: 5.9487 - val_accuracy: 0.3654 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.4832 - accuracy: 0.5337 - val_loss: 5.7596 - val_accuracy: 0.4808 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 6.3587 - accuracy: 0.5409 - val_loss: 5.5049 - val_accuracy: 0.4904 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 6.0557 - accuracy: 0.5817 - val_loss: 5.4855 - val_accuracy: 0.4808 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.9887 - accuracy: 0.5865 - val_loss: 5.4920 - val_accuracy: 0.4808 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.8484 - accuracy: 0.6298 - val_loss: 5.1465 - val_accuracy: 0.5962 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 5.9027 - accuracy: 0.5769 - val_loss: 4.9896 - val_accuracy: 0.6154 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 5.7296 - accuracy: 0.5986 - val_loss: 4.8436 - val_accuracy: 0.6827 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.3962 - accuracy: 0.7067 - val_loss: 4.7828 - val_accuracy: 0.6827 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.4612 - accuracy: 0.6659 - val_loss: 4.7588 - val_accuracy: 0.7115 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 2s 75ms/step - loss: 5.3406 - accuracy: 0.6947 - val_loss: 4.7194 - val_accuracy: 0.6731 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 5.2385 - accuracy: 0.6923 - val_loss: 4.8013 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 5.1904 - accuracy: 0.7115 - val_loss: 4.7116 - val_accuracy: 0.6731 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 5.1275 - accuracy: 0.7019 - val_loss: 4.5907 - val_accuracy: 0.7115 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 5.0128 - accuracy: 0.7091 - val_loss: 4.5410 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 4.8452 - accuracy: 0.7284 - val_loss: 4.5558 - val_accuracy: 0.6923 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 4.9586 - accuracy: 0.7043 - val_loss: 4.3415 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.7282 - accuracy: 0.7740 - val_loss: 4.2973 - val_accuracy: 0.7500 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.6208 - accuracy: 0.7788 - val_loss: 4.2564 - val_accuracy: 0.7500 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.7906 - accuracy: 0.7404 - val_loss: 4.3235 - val_accuracy: 0.7500 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.5360 - accuracy: 0.7957 - val_loss: 4.3874 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 4.4796 - accuracy: 0.7933 - val_loss: 4.2760 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 4.3811 - accuracy: 0.8101 - val_loss: 4.0703 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 4.3690 - accuracy: 0.7885 - val_loss: 4.0294 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.2357 - accuracy: 0.8173 - val_loss: 4.1253 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 4.1111 - accuracy: 0.8293 - val_loss: 4.1702 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.2396 - accuracy: 0.8269 - val_loss: 3.9652 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.0779 - accuracy: 0.8389 - val_loss: 3.9659 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.1083 - accuracy: 0.8197 - val_loss: 3.9512 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.9185 - accuracy: 0.8726 - val_loss: 3.7488 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.8902 - accuracy: 0.8654 - val_loss: 3.8764 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.8387 - accuracy: 0.8750 - val_loss: 3.7101 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.8447 - accuracy: 0.8486 - val_loss: 3.7668 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 3.8452 - accuracy: 0.8582 - val_loss: 3.8460 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.9216 - accuracy: 0.8534 - val_loss: 3.8770 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.8217 - accuracy: 0.8510 - val_loss: 3.8580 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 3.7934 - accuracy: 0.8341 - val_loss: 3.7002 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 2s 73ms/step - loss: 3.6401 - accuracy: 0.8678 - val_loss: 3.7017 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.7620 - accuracy: 0.8534 - val_loss: 3.9406 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.6464 - accuracy: 0.8798 - val_loss: 3.5077 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.4894 - accuracy: 0.8990 - val_loss: 3.4562 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.4080 - accuracy: 0.9111 - val_loss: 3.2799 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.4192 - accuracy: 0.8846 - val_loss: 3.6367 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.3340 - accuracy: 0.9087 - val_loss: 3.1868 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.3853 - accuracy: 0.8822 - val_loss: 3.5206 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.2430 - accuracy: 0.9111 - val_loss: 3.1566 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.2425 - accuracy: 0.9111 - val_loss: 3.0622 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.2772 - accuracy: 0.9111 - val_loss: 3.1141 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1483 - accuracy: 0.9087 - val_loss: 3.0238 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.0396 - accuracy: 0.9351 - val_loss: 2.9885 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.1549 - accuracy: 0.9135 - val_loss: 3.0094 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.0912 - accuracy: 0.9279 - val_loss: 3.4754 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.1102 - accuracy: 0.9087 - val_loss: 3.0389 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.0472 - accuracy: 0.9087 - val_loss: 2.8470 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.1080 - accuracy: 0.8798 - val_loss: 2.8873 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.9920 - accuracy: 0.9327 - val_loss: 3.0638 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.9177 - accuracy: 0.9375 - val_loss: 3.0958 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.8531 - accuracy: 0.9327 - val_loss: 2.8067 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.7606 - accuracy: 0.9519 - val_loss: 2.7465 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.7771 - accuracy: 0.9231 - val_loss: 2.7247 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.7010 - accuracy: 0.9447 - val_loss: 2.7353 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.6320 - accuracy: 0.9567 - val_loss: 2.7262 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.6152 - accuracy: 0.9567 - val_loss: 2.6988 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.6463 - accuracy: 0.9447 - val_loss: 2.6411 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 2s 75ms/step - loss: 2.7930 - accuracy: 0.9014 - val_loss: 2.6028 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.6468 - accuracy: 0.9303 - val_loss: 2.9342 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.5959 - accuracy: 0.9495 - val_loss: 2.6095 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.7180 - accuracy: 0.8990 - val_loss: 2.8941 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.5184 - accuracy: 0.9351 - val_loss: 2.6067 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.4810 - accuracy: 0.9495\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.4810 - accuracy: 0.9495 - val_loss: 2.6422 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.3668 - accuracy: 0.9615 - val_loss: 2.6110 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.3779 - accuracy: 0.9615 - val_loss: 2.4707 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.3673 - accuracy: 0.9519 - val_loss: 2.4671 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.3791 - accuracy: 0.9615 - val_loss: 2.5314 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.3922 - accuracy: 0.9423 - val_loss: 2.4906 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 2.3261 - accuracy: 0.9591 - val_loss: 2.4386 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.2904 - accuracy: 0.9712 - val_loss: 2.4414 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.2913 - accuracy: 0.9663 - val_loss: 2.4538 - val_accuracy: 0.8846 - lr: 1.5000e-04\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.2742 - accuracy: 0.9567 - val_loss: 2.4052 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.2224 - accuracy: 0.9760 - val_loss: 2.3349 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.2010 - accuracy: 0.9688 - val_loss: 2.3190 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.2110 - accuracy: 0.9663 - val_loss: 2.3338 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.2392 - accuracy: 0.9688 - val_loss: 2.4596 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.2703 - accuracy: 0.9543 - val_loss: 2.2852 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.2285 - accuracy: 0.9471 - val_loss: 2.2572 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.1583 - accuracy: 0.9663 - val_loss: 2.2839 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.1272 - accuracy: 0.9760 - val_loss: 2.3081 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 2.1466 - accuracy: 0.9663 - val_loss: 2.2844 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.1460 - accuracy: 0.9736 - val_loss: 2.3016 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.0835 - accuracy: 0.9784 - val_loss: 2.2059 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.0869 - accuracy: 0.9784 - val_loss: 2.2871 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.0572 - accuracy: 0.9784 - val_loss: 2.1913 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 2s 74ms/step - loss: 2.1488 - accuracy: 0.9663 - val_loss: 2.1533 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.0669 - accuracy: 0.9808 - val_loss: 2.2033 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.0097 - accuracy: 0.9784 - val_loss: 2.2339 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.0059 - accuracy: 0.9808 - val_loss: 2.2635 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "4/4 [==============================] - 1s 17ms/step\n",
      "\n",
      "Training Fold 2/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 24 samples\n",
      "hammer curl: 7 samples\n",
      "lat pulldown: 19 samples\n",
      "lateral raise: 15 samples\n",
      "pull Up: 10 samples\n",
      "push-up: 22 samples\n",
      "shoulder press: 7 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 45, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 22, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 22, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 7s 87ms/step - loss: 8.3346 - accuracy: 0.2428 - val_loss: 6.4823 - val_accuracy: 0.0769 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 7.5378 - accuracy: 0.3100\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 7.5105 - accuracy: 0.3149 - val_loss: 6.4739 - val_accuracy: 0.0865 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 6.9698 - accuracy: 0.4275\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 6.9577 - accuracy: 0.4303 - val_loss: 6.4005 - val_accuracy: 0.1058 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 6.7362 - accuracy: 0.4425\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 6.7110 - accuracy: 0.4495 - val_loss: 6.2920 - val_accuracy: 0.1731 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 6.5148 - accuracy: 0.5000\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 6.5138 - accuracy: 0.5000 - val_loss: 6.0979 - val_accuracy: 0.2500 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 6.4047 - accuracy: 0.5100\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.4131 - accuracy: 0.5048 - val_loss: 5.9006 - val_accuracy: 0.2981 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 6.3699 - accuracy: 0.4736 - val_loss: 5.7580 - val_accuracy: 0.3750 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 5.9918 - accuracy: 0.5673 - val_loss: 5.4968 - val_accuracy: 0.4327 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 6.0230 - accuracy: 0.5673 - val_loss: 5.3777 - val_accuracy: 0.5192 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 5.9595 - accuracy: 0.5721 - val_loss: 5.1292 - val_accuracy: 0.6442 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 5.8166 - accuracy: 0.5433 - val_loss: 4.9790 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 5.5856 - accuracy: 0.6274 - val_loss: 4.8614 - val_accuracy: 0.6827 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 5.5468 - accuracy: 0.6274 - val_loss: 4.7869 - val_accuracy: 0.6731 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 5.5012 - accuracy: 0.6106 - val_loss: 4.6321 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 2s 77ms/step - loss: 5.2899 - accuracy: 0.6611 - val_loss: 4.7180 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 5.2881 - accuracy: 0.6683 - val_loss: 4.5181 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 5.1577 - accuracy: 0.7067 - val_loss: 4.6796 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 5.1637 - accuracy: 0.6683 - val_loss: 4.5010 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 4.9435 - accuracy: 0.7091 - val_loss: 4.3492 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 4.8762 - accuracy: 0.7404 - val_loss: 4.3863 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 4.8440 - accuracy: 0.7236 - val_loss: 4.3221 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 4.7026 - accuracy: 0.7596 - val_loss: 4.4598 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 4.6731 - accuracy: 0.7212 - val_loss: 4.3048 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 4.4487 - accuracy: 0.7788 - val_loss: 4.2647 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 4.4793 - accuracy: 0.7812 - val_loss: 4.0750 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.4565 - accuracy: 0.7861 - val_loss: 4.0420 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 4.3618 - accuracy: 0.8197 - val_loss: 3.9832 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.2385 - accuracy: 0.8293 - val_loss: 3.9726 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 4.2955 - accuracy: 0.8005 - val_loss: 4.0074 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 4.1897 - accuracy: 0.8173 - val_loss: 3.7865 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.0573 - accuracy: 0.8558 - val_loss: 3.8279 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 4.0981 - accuracy: 0.8221 - val_loss: 3.8120 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.8727 - accuracy: 0.8486 - val_loss: 3.7840 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.9371 - accuracy: 0.8221 - val_loss: 3.6547 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.7334 - accuracy: 0.8774 - val_loss: 3.4754 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.8082 - accuracy: 0.8630 - val_loss: 3.5498 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.6805 - accuracy: 0.8726 - val_loss: 3.7076 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 3.8781 - accuracy: 0.8413 - val_loss: 3.5480 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.7423 - accuracy: 0.8486 - val_loss: 3.6908 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 3.6759 - accuracy: 0.8575\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.6930 - accuracy: 0.8510 - val_loss: 3.6103 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 3.5436 - accuracy: 0.8846 - val_loss: 3.4830 - val_accuracy: 0.8462 - lr: 1.5000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 1s 30ms/step - loss: 3.4326 - accuracy: 0.9062 - val_loss: 3.3302 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.4584 - accuracy: 0.8966 - val_loss: 3.3211 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.4594 - accuracy: 0.9038 - val_loss: 3.6518 - val_accuracy: 0.7981 - lr: 1.5000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.4263 - accuracy: 0.8894 - val_loss: 3.3485 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.3336 - accuracy: 0.8894 - val_loss: 3.2995 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.2226 - accuracy: 0.9279 - val_loss: 3.2065 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.2683 - accuracy: 0.9399 - val_loss: 3.2419 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.3660 - accuracy: 0.8966 - val_loss: 3.1870 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.2216 - accuracy: 0.9351 - val_loss: 3.1965 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1884 - accuracy: 0.9183 - val_loss: 3.1337 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.1524 - accuracy: 0.9351 - val_loss: 3.0872 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1637 - accuracy: 0.9183 - val_loss: 3.1859 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1018 - accuracy: 0.9423 - val_loss: 3.1201 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.0092 - accuracy: 0.9543 - val_loss: 3.0629 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.0182 - accuracy: 0.9495 - val_loss: 3.0465 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.0500 - accuracy: 0.9423 - val_loss: 3.0183 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1126 - accuracy: 0.9207 - val_loss: 3.0391 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.9815 - accuracy: 0.9447 - val_loss: 2.9629 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.9291 - accuracy: 0.9471 - val_loss: 2.9413 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.9674 - accuracy: 0.9351 - val_loss: 2.8928 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.9773 - accuracy: 0.9327 - val_loss: 2.9731 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.9505 - accuracy: 0.9375 - val_loss: 2.9461 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.0184 - accuracy: 0.9207 - val_loss: 3.0365 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.8857 - accuracy: 0.9375 - val_loss: 2.9696 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.8598 - accuracy: 0.9519\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 2.8598 - accuracy: 0.9519 - val_loss: 2.9161 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 2.8673 - accuracy: 0.9471 - val_loss: 2.8257 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 2.8484 - accuracy: 0.9375 - val_loss: 2.8175 - val_accuracy: 0.9327 - lr: 7.5000e-05\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.7753 - accuracy: 0.9519 - val_loss: 2.8393 - val_accuracy: 0.9327 - lr: 7.5000e-05\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.8010 - accuracy: 0.9471 - val_loss: 2.8435 - val_accuracy: 0.9327 - lr: 7.5000e-05\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.9480 - accuracy: 0.9159 - val_loss: 2.9067 - val_accuracy: 0.8942 - lr: 7.5000e-05\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.7910 - accuracy: 0.9351 - val_loss: 2.7858 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.7657 - accuracy: 0.9519 - val_loss: 2.7678 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.6604 - accuracy: 0.9760 - val_loss: 2.7545 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.6475 - accuracy: 0.9688 - val_loss: 2.7620 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.6603 - accuracy: 0.9688 - val_loss: 2.7232 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.7137 - accuracy: 0.9567 - val_loss: 2.7080 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.6647 - accuracy: 0.9519 - val_loss: 2.7193 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.6319 - accuracy: 0.9784 - val_loss: 2.6934 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.6564 - accuracy: 0.9567 - val_loss: 2.7393 - val_accuracy: 0.9231 - lr: 7.5000e-05\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.5767 - accuracy: 0.9736 - val_loss: 2.6973 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.6497 - accuracy: 0.9591 - val_loss: 2.6936 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.6831 - accuracy: 0.9543 - val_loss: 2.6495 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.6020 - accuracy: 0.9712 - val_loss: 2.6414 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.5727 - accuracy: 0.9688 - val_loss: 2.6287 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.6213 - accuracy: 0.9639 - val_loss: 2.6019 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.5333 - accuracy: 0.9712 - val_loss: 2.5969 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.4927 - accuracy: 0.9808 - val_loss: 2.6110 - val_accuracy: 0.9327 - lr: 7.5000e-05\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.6029 - accuracy: 0.9471 - val_loss: 2.5829 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.5049 - accuracy: 0.9712 - val_loss: 2.6243 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.5710 - accuracy: 0.9519 - val_loss: 2.6191 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.4793 - accuracy: 0.9663 - val_loss: 2.5976 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 2s 78ms/step - loss: 2.5314 - accuracy: 0.9736 - val_loss: 2.5607 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 2.4575 - accuracy: 0.9712 - val_loss: 2.5455 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 2.4916 - accuracy: 0.9567 - val_loss: 2.5812 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 2.4555 - accuracy: 0.9639 - val_loss: 2.5279 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.4946 - accuracy: 0.9567 - val_loss: 2.5894 - val_accuracy: 0.9135 - lr: 7.5000e-05\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.4181 - accuracy: 0.9712 - val_loss: 2.5512 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.4250 - accuracy: 0.9736 - val_loss: 2.5749 - val_accuracy: 0.9231 - lr: 7.5000e-05\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.3990 - accuracy: 0.9760 - val_loss: 2.5736 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "4/4 [==============================] - 1s 13ms/step\n",
      "\n",
      "Training Fold 3/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 24 samples\n",
      "hammer curl: 7 samples\n",
      "lat pulldown: 19 samples\n",
      "lateral raise: 14 samples\n",
      "pull Up: 11 samples\n",
      "push-up: 22 samples\n",
      "shoulder press: 7 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_2 (Conv1D)           (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 45, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 22, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 22, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 7s 95ms/step - loss: 8.7960 - accuracy: 0.1971 - val_loss: 6.4090 - val_accuracy: 0.1538 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 7.6432 - accuracy: 0.2740 - val_loss: 6.2918 - val_accuracy: 0.2212 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 7.3534 - accuracy: 0.3630 - val_loss: 6.1760 - val_accuracy: 0.2885 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 6.9901 - accuracy: 0.4111 - val_loss: 6.0016 - val_accuracy: 0.3462 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 6.5611 - accuracy: 0.4712 - val_loss: 5.7693 - val_accuracy: 0.4423 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 6.5311 - accuracy: 0.4784 - val_loss: 5.5333 - val_accuracy: 0.5865 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.1685 - accuracy: 0.5625 - val_loss: 5.2739 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 6.2554 - accuracy: 0.4976 - val_loss: 5.1593 - val_accuracy: 0.6442 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 6.0309 - accuracy: 0.5361 - val_loss: 5.0942 - val_accuracy: 0.5865 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 2s 81ms/step - loss: 5.9784 - accuracy: 0.5505 - val_loss: 5.0451 - val_accuracy: 0.6250 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.8070 - accuracy: 0.6010 - val_loss: 4.9296 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.8853 - accuracy: 0.5625 - val_loss: 4.8128 - val_accuracy: 0.6250 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 5.6727 - accuracy: 0.5913 - val_loss: 4.7538 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.5461 - accuracy: 0.6226 - val_loss: 4.6308 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 5.5368 - accuracy: 0.6130 - val_loss: 4.5310 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 5.4213 - accuracy: 0.6322 - val_loss: 4.5228 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.3151 - accuracy: 0.6322 - val_loss: 4.6015 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 5.3418 - accuracy: 0.6490 - val_loss: 4.5492 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 5.1203 - accuracy: 0.6827 - val_loss: 4.5012 - val_accuracy: 0.6923 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.1932 - accuracy: 0.6707 - val_loss: 4.4280 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.1563 - accuracy: 0.6659 - val_loss: 4.2780 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 4.8925 - accuracy: 0.6971 - val_loss: 4.2324 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.8175 - accuracy: 0.7260 - val_loss: 4.1449 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 4.9999 - accuracy: 0.6923 - val_loss: 4.1639 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.7511 - accuracy: 0.7236 - val_loss: 4.1382 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.6798 - accuracy: 0.7428 - val_loss: 4.0966 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 4.6641 - accuracy: 0.7380 - val_loss: 4.1518 - val_accuracy: 0.6731 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.6411 - accuracy: 0.7428 - val_loss: 3.8795 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 4.5455 - accuracy: 0.7524 - val_loss: 3.8413 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 4.5463 - accuracy: 0.7476 - val_loss: 4.0882 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.4724 - accuracy: 0.7620 - val_loss: 3.7876 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.3497 - accuracy: 0.7812 - val_loss: 4.0919 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.2705 - accuracy: 0.7909 - val_loss: 3.9023 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 2s 80ms/step - loss: 4.2162 - accuracy: 0.7981 - val_loss: 3.6064 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.1082 - accuracy: 0.8053 - val_loss: 3.6085 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.1224 - accuracy: 0.8029 - val_loss: 3.7133 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.1729 - accuracy: 0.7548 - val_loss: 3.8992 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.9698 - accuracy: 0.8077 - val_loss: 3.4558 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 3.9644 - accuracy: 0.8053 - val_loss: 3.4657 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.8852 - accuracy: 0.8221 - val_loss: 3.4359 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.8370 - accuracy: 0.8269 - val_loss: 3.3509 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.7583 - accuracy: 0.8317 - val_loss: 3.5690 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 3.7002 - accuracy: 0.8486 - val_loss: 3.5227 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.7674 - accuracy: 0.8462 - val_loss: 3.2847 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.7254 - accuracy: 0.8486 - val_loss: 3.2105 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 3.6447 - accuracy: 0.8630 - val_loss: 3.3622 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 3.5524 - accuracy: 0.8606 - val_loss: 3.2506 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.5185 - accuracy: 0.8678 - val_loss: 3.1383 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.5622 - accuracy: 0.8630 - val_loss: 3.1082 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 3.5040 - accuracy: 0.8606 - val_loss: 3.1441 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.4375 - accuracy: 0.8486 - val_loss: 3.2810 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.3050 - accuracy: 0.8678 - val_loss: 3.0742 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.3056 - accuracy: 0.8870 - val_loss: 2.9321 - val_accuracy: 0.9615 - lr: 3.0000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.3189 - accuracy: 0.8678 - val_loss: 3.1040 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 3.1201 - accuracy: 0.9087 - val_loss: 2.9575 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.2531 - accuracy: 0.8678 - val_loss: 3.0429 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.0686 - accuracy: 0.9014 - val_loss: 2.8105 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 2s 85ms/step - loss: 3.1401 - accuracy: 0.8894 - val_loss: 2.7974 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1504 - accuracy: 0.8726 - val_loss: 2.8433 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.1154 - accuracy: 0.8990 - val_loss: 2.8157 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.0285 - accuracy: 0.9062 - val_loss: 3.2258 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.0288 - accuracy: 0.8822 - val_loss: 2.8972 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 63/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 2.9473 - accuracy: 0.9100\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.9479 - accuracy: 0.9087 - val_loss: 2.9221 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.8817 - accuracy: 0.9183 - val_loss: 2.6808 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.8784 - accuracy: 0.9207 - val_loss: 2.6454 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.8599 - accuracy: 0.9111 - val_loss: 2.5707 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.8498 - accuracy: 0.9159 - val_loss: 2.5802 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.7297 - accuracy: 0.9207 - val_loss: 2.5577 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.7266 - accuracy: 0.9351 - val_loss: 2.5185 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.8310 - accuracy: 0.9231 - val_loss: 2.5925 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.6783 - accuracy: 0.9519 - val_loss: 2.5407 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 2.7715 - accuracy: 0.9255 - val_loss: 2.6944 - val_accuracy: 0.8750 - lr: 1.5000e-04\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.7205 - accuracy: 0.9375 - val_loss: 2.4812 - val_accuracy: 0.9808 - lr: 1.5000e-04\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.6118 - accuracy: 0.9399 - val_loss: 2.5120 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.6415 - accuracy: 0.9471 - val_loss: 2.5315 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.8736 - accuracy: 0.9087 - val_loss: 2.5304 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.6778 - accuracy: 0.9423 - val_loss: 2.4656 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.6417 - accuracy: 0.9447 - val_loss: 2.4082 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.5870 - accuracy: 0.9327 - val_loss: 2.4080 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.5463 - accuracy: 0.9495 - val_loss: 2.4521 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.4634 - accuracy: 0.9639 - val_loss: 2.4000 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 2.5599 - accuracy: 0.9375 - val_loss: 2.4436 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 2.6114 - accuracy: 0.9351 - val_loss: 2.4061 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.4821 - accuracy: 0.9519 - val_loss: 2.3771 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.4282 - accuracy: 0.9712 - val_loss: 2.5440 - val_accuracy: 0.8750 - lr: 1.5000e-04\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.5850 - accuracy: 0.9351 - val_loss: 2.2967 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.4272 - accuracy: 0.9471 - val_loss: 2.2572 - val_accuracy: 0.9904 - lr: 1.5000e-04\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.3929 - accuracy: 0.9495 - val_loss: 2.2796 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.3985 - accuracy: 0.9591 - val_loss: 2.3045 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.3480 - accuracy: 0.9688 - val_loss: 2.2603 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.3719 - accuracy: 0.9688 - val_loss: 2.2931 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.3763 - accuracy: 0.9543 - val_loss: 2.1857 - val_accuracy: 0.9808 - lr: 1.5000e-04\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.4373 - accuracy: 0.9495 - val_loss: 2.3813 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.3295 - accuracy: 0.9615 - val_loss: 2.1684 - val_accuracy: 0.9808 - lr: 1.5000e-04\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.2692 - accuracy: 0.9712 - val_loss: 2.2296 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.2792 - accuracy: 0.9639 - val_loss: 2.2032 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.3246 - accuracy: 0.9591 - val_loss: 2.2230 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.2995 - accuracy: 0.9495 - val_loss: 2.1540 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.2323 - accuracy: 0.9784 - val_loss: 2.1579 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.2838 - accuracy: 0.9615 - val_loss: 2.0976 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "4/4 [==============================] - 1s 15ms/step\n",
      "\n",
      "Training Fold 4/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 23 samples\n",
      "hammer curl: 8 samples\n",
      "lat pulldown: 18 samples\n",
      "lateral raise: 15 samples\n",
      "pull Up: 11 samples\n",
      "push-up: 22 samples\n",
      "shoulder press: 7 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 45, 64)            256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPoolin  (None, 22, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 22, 128)           512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 8s 96ms/step - loss: 8.3881 - accuracy: 0.2115 - val_loss: 6.3557 - val_accuracy: 0.2788 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 7.3998 - accuracy: 0.4087 - val_loss: 6.2439 - val_accuracy: 0.4135 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 7.0557 - accuracy: 0.4519 - val_loss: 6.1066 - val_accuracy: 0.4423 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 6.7345 - accuracy: 0.4615 - val_loss: 5.9146 - val_accuracy: 0.5481 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 6.4341 - accuracy: 0.5264 - val_loss: 5.7229 - val_accuracy: 0.5962 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 6.2349 - accuracy: 0.5673 - val_loss: 5.5257 - val_accuracy: 0.6250 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 6.1420 - accuracy: 0.5841 - val_loss: 5.3770 - val_accuracy: 0.6154 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 5.9957 - accuracy: 0.6058 - val_loss: 5.2542 - val_accuracy: 0.6058 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.0472 - accuracy: 0.5841 - val_loss: 5.0627 - val_accuracy: 0.6635 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.8969 - accuracy: 0.5625 - val_loss: 4.9008 - val_accuracy: 0.6923 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 5.5558 - accuracy: 0.6490 - val_loss: 4.7932 - val_accuracy: 0.6635 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.5543 - accuracy: 0.6851 - val_loss: 4.7159 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.3950 - accuracy: 0.6803 - val_loss: 4.6323 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.2950 - accuracy: 0.7139 - val_loss: 4.6146 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 5.0876 - accuracy: 0.7284 - val_loss: 4.5158 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.0227 - accuracy: 0.7188 - val_loss: 4.4594 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.0756 - accuracy: 0.6947 - val_loss: 4.3992 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.0189 - accuracy: 0.6779 - val_loss: 4.4143 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.8592 - accuracy: 0.7572 - val_loss: 4.2228 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 4.7761 - accuracy: 0.7284 - val_loss: 4.3618 - val_accuracy: 0.7115 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.7194 - accuracy: 0.7572 - val_loss: 4.1801 - val_accuracy: 0.7115 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.6566 - accuracy: 0.7620 - val_loss: 4.1614 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.4965 - accuracy: 0.7861 - val_loss: 3.9468 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.5546 - accuracy: 0.7764 - val_loss: 3.9906 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 4.3854 - accuracy: 0.7933 - val_loss: 4.1811 - val_accuracy: 0.6923 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 4.3853 - accuracy: 0.7933 - val_loss: 4.1309 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 4.2047 - accuracy: 0.8269 - val_loss: 4.1283 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 4.2911 - accuracy: 0.7957 - val_loss: 3.9191 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 4.2304 - accuracy: 0.8077 - val_loss: 3.9197 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 4.2235 - accuracy: 0.7981 - val_loss: 3.9222 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.1482 - accuracy: 0.8053 - val_loss: 3.8246 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 4.2505 - accuracy: 0.7740 - val_loss: 3.8051 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.9977 - accuracy: 0.8053 - val_loss: 4.0004 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.9128 - accuracy: 0.8221 - val_loss: 3.6767 - val_accuracy: 0.7500 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 4.0724 - accuracy: 0.7837 - val_loss: 3.6177 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.8207 - accuracy: 0.8438 - val_loss: 3.3769 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.7124 - accuracy: 0.8654 - val_loss: 3.4019 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.7443 - accuracy: 0.8486 - val_loss: 3.6399 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.5945 - accuracy: 0.8534 - val_loss: 3.5452 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 3.5867 - accuracy: 0.8726 - val_loss: 3.4655 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 3.6416 - accuracy: 0.8606 - val_loss: 3.2517 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 3.5076 - accuracy: 0.8750 - val_loss: 3.3508 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.5767 - accuracy: 0.8606 - val_loss: 3.2005 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.3650 - accuracy: 0.8870 - val_loss: 3.1730 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.3674 - accuracy: 0.8846 - val_loss: 3.0990 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 3.2222 - accuracy: 0.9014 - val_loss: 3.0636 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 3.3578 - accuracy: 0.8510 - val_loss: 3.1561 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.1621 - accuracy: 0.9183 - val_loss: 2.9586 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.2555 - accuracy: 0.8918 - val_loss: 3.1639 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 2s 82ms/step - loss: 3.2159 - accuracy: 0.8966 - val_loss: 2.9341 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.0846 - accuracy: 0.9087 - val_loss: 2.9757 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1680 - accuracy: 0.8990 - val_loss: 2.9347 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.1203 - accuracy: 0.8870 - val_loss: 3.1183 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.0816 - accuracy: 0.8918 - val_loss: 2.8741 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.0258 - accuracy: 0.8918 - val_loss: 2.8711 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.0027 - accuracy: 0.8990 - val_loss: 3.1049 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.9048 - accuracy: 0.9231 - val_loss: 3.0994 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.8509 - accuracy: 0.9062 - val_loss: 2.6930 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.7668 - accuracy: 0.9327 - val_loss: 2.9736 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.8996 - accuracy: 0.9159 - val_loss: 3.0401 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.0447 - accuracy: 0.8654 - val_loss: 2.6349 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.7469 - accuracy: 0.9231 - val_loss: 2.5498 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.8008 - accuracy: 0.8966 - val_loss: 2.4758 - val_accuracy: 0.9423 - lr: 3.0000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.7127 - accuracy: 0.9207 - val_loss: 2.4542 - val_accuracy: 0.9423 - lr: 3.0000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.6972 - accuracy: 0.9062 - val_loss: 2.4953 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.5930 - accuracy: 0.9159 - val_loss: 2.4933 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.6094 - accuracy: 0.9327 - val_loss: 2.5730 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.6188 - accuracy: 0.9111 - val_loss: 2.4342 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.5009 - accuracy: 0.9375 - val_loss: 2.3349 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.5284 - accuracy: 0.9351 - val_loss: 2.4067 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.6216 - accuracy: 0.9183 - val_loss: 2.5203 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.4235 - accuracy: 0.9279 - val_loss: 2.3858 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.4253 - accuracy: 0.9327 - val_loss: 2.4748 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 74/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 2.4399 - accuracy: 0.9225\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.4391 - accuracy: 0.9231 - val_loss: 2.5201 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 2.3575 - accuracy: 0.9375 - val_loss: 2.2708 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 2s 56ms/step - loss: 2.2746 - accuracy: 0.9543 - val_loss: 2.2068 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.3503 - accuracy: 0.9327 - val_loss: 2.1339 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.3335 - accuracy: 0.9471 - val_loss: 2.1527 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.3991 - accuracy: 0.9279 - val_loss: 2.3028 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.4379 - accuracy: 0.9207 - val_loss: 2.3210 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.3012 - accuracy: 0.9207 - val_loss: 2.3014 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.3106 - accuracy: 0.9375 - val_loss: 2.1256 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.2897 - accuracy: 0.9423 - val_loss: 2.1741 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.2221 - accuracy: 0.9495 - val_loss: 2.0989 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.1945 - accuracy: 0.9591 - val_loss: 2.2334 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.1860 - accuracy: 0.9519 - val_loss: 2.1280 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.0881 - accuracy: 0.9760 - val_loss: 2.1147 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.0852 - accuracy: 0.9712 - val_loss: 2.0531 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.1501 - accuracy: 0.9615 - val_loss: 2.1114 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.0634 - accuracy: 0.9688 - val_loss: 2.1019 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.0587 - accuracy: 0.9688 - val_loss: 2.0277 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.0964 - accuracy: 0.9591 - val_loss: 2.0312 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.0195 - accuracy: 0.9760 - val_loss: 2.0318 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.0095 - accuracy: 0.9688 - val_loss: 2.0490 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.0939 - accuracy: 0.9375 - val_loss: 2.0643 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 96/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 1.9976 - accuracy: 0.9550\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 1.9936 - accuracy: 0.9567 - val_loss: 2.0879 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.0211 - accuracy: 0.9567 - val_loss: 2.0425 - val_accuracy: 0.9231 - lr: 7.5000e-05\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 1.9682 - accuracy: 0.9663 - val_loss: 1.9650 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 1.9830 - accuracy: 0.9639 - val_loss: 1.9552 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 1.8981 - accuracy: 0.9856 - val_loss: 1.9398 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "4/4 [==============================] - 2s 17ms/step\n",
      "\n",
      "Training Fold 5/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 23 samples\n",
      "hammer curl: 8 samples\n",
      "lat pulldown: 18 samples\n",
      "lateral raise: 15 samples\n",
      "pull Up: 10 samples\n",
      "push-up: 23 samples\n",
      "shoulder press: 7 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_4 (Conv1D)           (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 45, 64)            256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPoolin  (None, 22, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 22, 128)           512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 8s 103ms/step - loss: 8.2276 - accuracy: 0.2692 - val_loss: 6.4020 - val_accuracy: 0.1827 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 7.6124 - accuracy: 0.3870 - val_loss: 6.3455 - val_accuracy: 0.2404 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 2s 72ms/step - loss: 7.1872 - accuracy: 0.3678 - val_loss: 6.2257 - val_accuracy: 0.2885 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 6.9014 - accuracy: 0.4615 - val_loss: 6.1054 - val_accuracy: 0.3077 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 6.5610 - accuracy: 0.4904 - val_loss: 5.8927 - val_accuracy: 0.3846 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 6.5610 - accuracy: 0.5144 - val_loss: 5.7603 - val_accuracy: 0.4231 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 6.4113 - accuracy: 0.5553 - val_loss: 5.4997 - val_accuracy: 0.4904 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 2s 74ms/step - loss: 6.0164 - accuracy: 0.5913 - val_loss: 5.2499 - val_accuracy: 0.5962 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 5.8103 - accuracy: 0.5913 - val_loss: 5.1494 - val_accuracy: 0.6250 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 2s 69ms/step - loss: 5.7697 - accuracy: 0.5913 - val_loss: 5.0929 - val_accuracy: 0.5481 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 3s 103ms/step - loss: 5.7347 - accuracy: 0.6058 - val_loss: 4.8247 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 5.5697 - accuracy: 0.6130 - val_loss: 4.6947 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 5.5006 - accuracy: 0.6442 - val_loss: 4.7143 - val_accuracy: 0.6827 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 5.2184 - accuracy: 0.6995 - val_loss: 4.5071 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 5.1809 - accuracy: 0.6947 - val_loss: 4.6031 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 5.0721 - accuracy: 0.7043 - val_loss: 4.5004 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 4.9932 - accuracy: 0.7139 - val_loss: 4.6308 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 4.9941 - accuracy: 0.7212 - val_loss: 4.4630 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 4.8575 - accuracy: 0.7139 - val_loss: 4.2956 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 4.8687 - accuracy: 0.6899 - val_loss: 4.4181 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 4.8252 - accuracy: 0.7043 - val_loss: 4.2152 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 4.6576 - accuracy: 0.7260 - val_loss: 4.2154 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 4.4949 - accuracy: 0.7716 - val_loss: 4.1442 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 4.5054 - accuracy: 0.7909 - val_loss: 4.0996 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 4.3869 - accuracy: 0.7668 - val_loss: 4.0213 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 4.3467 - accuracy: 0.7933 - val_loss: 4.1677 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 3s 101ms/step - loss: 4.3275 - accuracy: 0.7837 - val_loss: 3.9784 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 4.2198 - accuracy: 0.8101 - val_loss: 3.9118 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 4.1637 - accuracy: 0.8101 - val_loss: 3.8801 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 4.1325 - accuracy: 0.8077 - val_loss: 3.7791 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 4.0441 - accuracy: 0.8269 - val_loss: 3.6851 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 4.0011 - accuracy: 0.8197 - val_loss: 3.6433 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 3.9577 - accuracy: 0.8413 - val_loss: 3.9131 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 3.8478 - accuracy: 0.8462 - val_loss: 3.8994 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 3.7659 - accuracy: 0.8510 - val_loss: 3.7948 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.6632 - accuracy: 0.8534 - val_loss: 3.5244 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.6789 - accuracy: 0.8678 - val_loss: 3.6018 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.7629 - accuracy: 0.8317 - val_loss: 3.5280 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 3.7293 - accuracy: 0.8389 - val_loss: 3.4729 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.5328 - accuracy: 0.8702 - val_loss: 3.3426 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 3.6543 - accuracy: 0.8438 - val_loss: 3.3334 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.8079 - accuracy: 0.8149 - val_loss: 3.2619 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 2s 98ms/step - loss: 3.6209 - accuracy: 0.8462 - val_loss: 3.2497 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.3885 - accuracy: 0.8798 - val_loss: 3.2599 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 3.3462 - accuracy: 0.8798 - val_loss: 3.1708 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 3.2741 - accuracy: 0.8702 - val_loss: 3.1061 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 3.3211 - accuracy: 0.8558 - val_loss: 3.0011 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 3.1248 - accuracy: 0.9207 - val_loss: 3.1536 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.2363 - accuracy: 0.8822 - val_loss: 2.9042 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.2479 - accuracy: 0.8798 - val_loss: 2.9685 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 3.1565 - accuracy: 0.8798 - val_loss: 2.8532 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 3.1292 - accuracy: 0.8870 - val_loss: 3.0793 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.1024 - accuracy: 0.8870 - val_loss: 3.0475 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 3.0104 - accuracy: 0.8798 - val_loss: 2.8918 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.0575 - accuracy: 0.8966 - val_loss: 3.1440 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 2.9799 - accuracy: 0.8966 - val_loss: 2.8311 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.9646 - accuracy: 0.9014 - val_loss: 2.7248 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 2.8386 - accuracy: 0.9087 - val_loss: 2.6785 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 2s 69ms/step - loss: 2.8129 - accuracy: 0.9279 - val_loss: 2.6944 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 2s 82ms/step - loss: 2.8927 - accuracy: 0.8918 - val_loss: 2.6239 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 2.7027 - accuracy: 0.9087 - val_loss: 2.7748 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 2.6844 - accuracy: 0.9111 - val_loss: 2.6422 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 2.7688 - accuracy: 0.8822 - val_loss: 2.5718 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 2.6576 - accuracy: 0.9062 - val_loss: 2.5735 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 2.5847 - accuracy: 0.9399 - val_loss: 3.1485 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.7638 - accuracy: 0.8918 - val_loss: 2.7043 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.6141 - accuracy: 0.9207 - val_loss: 2.7068 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.6693 - accuracy: 0.9207 - val_loss: 2.5095 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 2.6179 - accuracy: 0.9303 - val_loss: 2.5805 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 2.5530 - accuracy: 0.9038 - val_loss: 2.4441 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 2.5236 - accuracy: 0.9231 - val_loss: 2.2946 - val_accuracy: 0.9519 - lr: 3.0000e-04\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.4263 - accuracy: 0.9159 - val_loss: 2.2184 - val_accuracy: 0.9615 - lr: 3.0000e-04\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.3906 - accuracy: 0.9279 - val_loss: 2.2285 - val_accuracy: 0.9615 - lr: 3.0000e-04\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.5317 - accuracy: 0.9159 - val_loss: 2.4557 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 2s 94ms/step - loss: 2.2917 - accuracy: 0.9255 - val_loss: 2.4104 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 2.3678 - accuracy: 0.9279 - val_loss: 2.2321 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 2.2688 - accuracy: 0.9399 - val_loss: 2.1968 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 2.2221 - accuracy: 0.9399 - val_loss: 2.5147 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 2.2164 - accuracy: 0.9423 - val_loss: 2.0469 - val_accuracy: 0.9519 - lr: 3.0000e-04\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.2728 - accuracy: 0.9447 - val_loss: 2.2368 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 2.3521 - accuracy: 0.8918 - val_loss: 2.3777 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.2986 - accuracy: 0.8990 - val_loss: 2.6526 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 2.2637 - accuracy: 0.9183 - val_loss: 2.2760 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.2350 - accuracy: 0.9159\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 2.2350 - accuracy: 0.9159 - val_loss: 2.0716 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 2.1895 - accuracy: 0.9231 - val_loss: 2.0354 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 2.1369 - accuracy: 0.9375 - val_loss: 1.9325 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.0475 - accuracy: 0.9591 - val_loss: 1.9004 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 2.0951 - accuracy: 0.9519 - val_loss: 1.9328 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.0034 - accuracy: 0.9543 - val_loss: 1.9423 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 2.1037 - accuracy: 0.9351 - val_loss: 2.0345 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 1.9841 - accuracy: 0.9495 - val_loss: 1.8736 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 3s 88ms/step - loss: 1.9610 - accuracy: 0.9591 - val_loss: 1.8527 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 1.9310 - accuracy: 0.9760 - val_loss: 2.1650 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 1.9049 - accuracy: 0.9688 - val_loss: 2.0286 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 1.9408 - accuracy: 0.9639 - val_loss: 1.8798 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 1.8710 - accuracy: 0.9808 - val_loss: 1.8657 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9696 - accuracy: 0.9615\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 1.9696 - accuracy: 0.9615 - val_loss: 2.4466 - val_accuracy: 0.8558 - lr: 1.5000e-04\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.0110 - accuracy: 0.9423 - val_loss: 2.0644 - val_accuracy: 0.8942 - lr: 7.5000e-05\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 1.8427 - accuracy: 0.9784 - val_loss: 1.9017 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 1.9836 - accuracy: 0.9471 - val_loss: 1.8902 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "4/4 [==============================] - 1s 21ms/step\n",
      "\n",
      "K-Fold Cross Validation Results (Enhanced CNN-LSTM with Weighted Features):\n",
      "Mean Validation Accuracy: 0.9442 ± 0.0185\n",
      "Mean Validation Loss: 2.1529 ± 0.2769\n",
      "Average Epochs per Fold: 100.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Updated paths to match new data location\n",
    "    data_dir = r\"D:\\minor\\workout_processed_data_landmark\"\n",
    "    save_path = r\"D:\\minor\\k_fold_CNN_LSTM_landmark\"\n",
    "    \n",
    "    trainer = WorkoutModelTrainer(data_dir)\n",
    "    fold_results, fold_histories = trainer.train_with_kfold(\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        n_splits=5,\n",
    "        save_path=save_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
