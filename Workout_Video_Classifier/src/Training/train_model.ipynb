{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Dropout, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkoutModelTrainer:\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.class_mapping = None\n",
    "        self.confidences = None\n",
    "        self.model = None\n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        print(\"Loading preprocessed data...\")\n",
    "        self.X = np.load(os.path.join(self.data_dir, 'X.npy'))\n",
    "        self.y = np.load(os.path.join(self.data_dir, 'y.npy'))\n",
    "        self.confidences = np.load(os.path.join(self.data_dir, 'confidences.npy'))\n",
    "        self.class_mapping = np.load(os.path.join(self.data_dir, 'class_mapping.npy'),\n",
    "                                   allow_pickle=True).item()\n",
    "        \n",
    "        print(f\"Loaded {len(self.X)} samples\")\n",
    "        print(f\"Input shape: {self.X.shape}\")  # Added to verify shape\n",
    "        print(\"\\nClass distribution:\")\n",
    "        for class_name, class_idx in self.class_mapping.items():\n",
    "            total = sum(1 for label in self.y if label == class_idx)\n",
    "            print(f\"{class_name}: {total} samples\")\n",
    "            \n",
    "    def build_model(self, input_shape, num_classes):\n",
    "        model = tf.keras.Sequential([\n",
    "            Conv1D(64, kernel_size=3, padding='same', activation='relu',\n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.01)), \n",
    "            BatchNormalization(),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "            Dropout(0.35),\n",
    "        \n",
    "            LSTM(128, return_sequences=True,\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.35),\n",
    "        \n",
    "            LSTM(64, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.35),\n",
    "        \n",
    "            Dense(64, activation='relu',\n",
    "                 kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.3),\n",
    "        \n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "    \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        # Print model summary with new input shape\n",
    "        print(\"\\nModel Summary:\")\n",
    "        model.build(input_shape=(None,) + input_shape)\n",
    "        model.summary()\n",
    "        \n",
    "        \n",
    "        return model\n",
    "        \n",
    "    def get_callbacks(self):\n",
    "        class OverfittingDetectionCallback(tf.keras.callbacks.Callback):\n",
    "            def on_epoch_end(self, epoch, logs={}):\n",
    "                if logs.get('accuracy') - logs.get('val_accuracy') > 0.2:\n",
    "                    print(\"\\nWarning: Possible overfitting detected!\")\n",
    "    \n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=12,\n",
    "                restore_best_weights=True,\n",
    "                min_delta=0.001\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            ),\n",
    "            OverfittingDetectionCallback()\n",
    "        ]\n",
    "    \n",
    "    def calculate_class_weights(self, y_train):\n",
    "        class_weights = {}\n",
    "        unique_classes, class_counts = np.unique(y_train, return_counts=True)\n",
    "        max_count = np.max(class_counts)\n",
    "        for cls, count in zip(unique_classes, class_counts):\n",
    "            class_weights[cls] = max_count / count\n",
    "        return class_weights\n",
    "\n",
    "    def save_training_plots(self, fold_histories, save_path):\n",
    "        plot_dir = os.path.join(save_path, 'training_plots')\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "        \n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        for fold, history in enumerate(fold_histories):\n",
    "            plt.plot(history['accuracy'], label=f'Train (Fold {fold+1})', alpha=0.3)\n",
    "            plt.plot(history['val_accuracy'], label=f'Val (Fold {fold+1})', alpha=0.3)\n",
    "        \n",
    "        plt.title('Model Accuracy Across Folds (Enhanced CNN-LSTM with Weighted Features)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        for fold, history in enumerate(fold_histories):\n",
    "            plt.plot(history['loss'], label=f'Train (Fold {fold+1})', alpha=0.3)\n",
    "            plt.plot(history['val_loss'], label=f'Val (Fold {fold+1})', alpha=0.3)\n",
    "        \n",
    "        plt.title('Model Loss Across Folds (Enhanced CNN-LSTM with Weighted Features)')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(plot_dir, 'combined_training_curves.png'), \n",
    "                   bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    def save_fold_results(self, fold_results, fold_histories, save_path):\n",
    "        results_dir = os.path.join(save_path, 'fold_results')\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        \n",
    "        all_metrics = []\n",
    "        for fold, (result, history) in enumerate(zip(fold_results, fold_histories)):\n",
    "            fold_dir = os.path.join(results_dir, f'fold_{fold+1}')\n",
    "            os.makedirs(fold_dir, exist_ok=True)\n",
    "            \n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(history['accuracy'], label='Training')\n",
    "            plt.plot(history['val_accuracy'], label='Validation')\n",
    "            plt.title(f'Model Accuracy - Fold {fold+1} (Enhanced CNN-LSTM with Weighted Features)')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.plot(history['loss'], label='Training')\n",
    "            plt.plot(history['val_loss'], label='Validation')\n",
    "            plt.title(f'Model Loss - Fold {fold+1} (Enhanced CNN-LSTM with Weighted Features)')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(fold_dir, 'training_curves.png'), \n",
    "                       bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            plt.figure(figsize=(15, 12))\n",
    "            sns.heatmap(result['confusion_matrix'], \n",
    "                       annot=True, \n",
    "                       fmt='d', \n",
    "                       cmap='Blues',\n",
    "                       xticklabels=list(self.class_mapping.keys()),\n",
    "                       yticklabels=list(self.class_mapping.keys()))\n",
    "            plt.title(f'Confusion Matrix - Fold {fold+1} (Enhanced CNN-LSTM with Weighted Features)')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(fold_dir, 'confusion_matrix.png'), \n",
    "                       bbox_inches='tight', dpi=300)\n",
    "            plt.close()\n",
    "            \n",
    "            report_df = pd.DataFrame(result['classification_report']).transpose()\n",
    "            report_df.to_csv(os.path.join(fold_dir, 'classification_report.csv'))\n",
    "            \n",
    "            report_txt = classification_report(\n",
    "                result['y_true'],\n",
    "                result['y_pred'],\n",
    "                target_names=list(self.class_mapping.keys()),\n",
    "                digits=2,\n",
    "                zero_division=0\n",
    "            )\n",
    "            with open(os.path.join(fold_dir, 'classification_report.txt'), 'w') as f:\n",
    "                f.write(report_txt)\n",
    "            \n",
    "            all_metrics.append({\n",
    "                'fold': fold + 1,\n",
    "                'val_accuracy': result['val_accuracy'],\n",
    "                'val_loss': result['val_loss'],\n",
    "                'best_epoch': len(history['loss'])\n",
    "            })\n",
    "        \n",
    "        metrics_df = pd.DataFrame(all_metrics)\n",
    "        metrics_df.to_csv(os.path.join(results_dir, 'all_fold_metrics.csv'), index=False)\n",
    "        \n",
    "        avg_metrics = {\n",
    "            'mean_val_accuracy': metrics_df['val_accuracy'].mean(),\n",
    "            'std_val_accuracy': metrics_df['val_accuracy'].std(),\n",
    "            'mean_val_loss': metrics_df['val_loss'].mean(),\n",
    "            'std_val_loss': metrics_df['val_loss'].std(),\n",
    "            'mean_epochs': metrics_df['best_epoch'].mean()\n",
    "        }\n",
    "        \n",
    "        pd.DataFrame([avg_metrics]).to_csv(\n",
    "            os.path.join(results_dir, 'average_metrics.csv'), index=False\n",
    "        )\n",
    "        \n",
    "        print(\"\\nK-Fold Cross Validation Results (Enhanced CNN-LSTM with Weighted Features):\")\n",
    "        print(f\"Mean Validation Accuracy: {avg_metrics['mean_val_accuracy']:.4f} ± {avg_metrics['std_val_accuracy']:.4f}\")\n",
    "        print(f\"Mean Validation Loss: {avg_metrics['mean_val_loss']:.4f} ± {avg_metrics['std_val_loss']:.4f}\")\n",
    "        print(f\"Average Epochs per Fold: {avg_metrics['mean_epochs']:.1f}\")\n",
    "\n",
    "    def train_with_kfold(self, epochs=100, batch_size=16, n_splits=5, save_path=None):\n",
    "        if save_path is None:\n",
    "            save_path = \"k_fold_CNN_LSTM_landmark\"\n",
    "    \n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        print(f\"\\nStarting {n_splits}-fold stratified cross validation with Enhanced CNN-LSTM architecture...\")\n",
    "        print(f\"Input shape per sample: {self.X.shape[1:]}\")\n",
    "        \n",
    "        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        fold_results = []\n",
    "        fold_histories = []\n",
    "    \n",
    "        for fold, (train_idx, val_idx) in enumerate(kfold.split(self.X, self.y)):\n",
    "            print(f\"\\nTraining Fold {fold+1}/{n_splits}\")\n",
    "        \n",
    "            X_train, X_val = self.X[train_idx], self.X[val_idx]\n",
    "            y_train, y_val = self.y[train_idx], self.y[val_idx]\n",
    "            \n",
    "            print(\"\\nClass distribution in validation set:\")\n",
    "            unique, counts = np.unique(y_val, return_counts=True)\n",
    "            for class_idx, count in zip(unique, counts):\n",
    "                class_name = list(self.class_mapping.keys())[list(self.class_mapping.values()).index(class_idx)]\n",
    "                print(f\"{class_name}: {count} samples\")\n",
    "        \n",
    "            input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "            model = self.build_model(input_shape, len(self.class_mapping))\n",
    "        \n",
    "            history = model.fit(\n",
    "                X_train, y_train,\n",
    "                validation_data=(X_val, y_val),\n",
    "                epochs=epochs,\n",
    "                batch_size=batch_size,\n",
    "                callbacks=self.get_callbacks(),\n",
    "                class_weight=self.calculate_class_weights(y_train),\n",
    "                verbose=1\n",
    "            )\n",
    "        \n",
    "            val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        \n",
    "            fold_results.append({\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'val_loss': val_loss,\n",
    "                'confusion_matrix': confusion_matrix(y_val, y_pred_classes),\n",
    "                'classification_report': classification_report(\n",
    "                    y_val, y_pred_classes,\n",
    "                    target_names=list(self.class_mapping.keys()),\n",
    "                    output_dict=True,\n",
    "                    zero_division=0\n",
    "                ),\n",
    "                'y_true': y_val,\n",
    "                'y_pred': y_pred_classes\n",
    "            })\n",
    "\n",
    "            fold_histories.append(history.history)\n",
    "            model.save(os.path.join(save_path, f'model_fold_{fold+1}.keras'))\n",
    "    \n",
    "        self.save_training_plots(fold_histories, save_path)\n",
    "        self.save_fold_results(fold_results, fold_histories, save_path)\n",
    "    \n",
    "        return fold_results, fold_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data...\n",
      "Loaded 520 samples\n",
      "Input shape: (520, 45, 297)\n",
      "\n",
      "Class distribution:\n",
      "barbell biceps curl: 118 samples\n",
      "hammer curl: 38 samples\n",
      "lat pulldown: 92 samples\n",
      "lateral raise: 74 samples\n",
      "pull Up: 52 samples\n",
      "push-up: 112 samples\n",
      "shoulder press: 34 samples\n",
      "\n",
      "Starting 5-fold stratified cross validation with Enhanced CNN-LSTM architecture...\n",
      "Input shape per sample: (45, 297)\n",
      "\n",
      "Training Fold 1/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 24 samples\n",
      "hammer curl: 8 samples\n",
      "lat pulldown: 18 samples\n",
      "lateral raise: 15 samples\n",
      "pull Up: 10 samples\n",
      "push-up: 23 samples\n",
      "shoulder press: 6 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 45, 64)            256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1  (None, 22, 64)            0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 22, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 8s 85ms/step - loss: 8.7183 - accuracy: 0.1899 - val_loss: 6.4138 - val_accuracy: 0.2115 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 7.6524 - accuracy: 0.3462 - val_loss: 6.3348 - val_accuracy: 0.2885 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 7.0924 - accuracy: 0.4375 - val_loss: 6.2176 - val_accuracy: 0.3173 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 7.0047 - accuracy: 0.4495 - val_loss: 6.1142 - val_accuracy: 0.3077 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.9052 - accuracy: 0.4207 - val_loss: 5.9487 - val_accuracy: 0.3654 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.4832 - accuracy: 0.5337 - val_loss: 5.7596 - val_accuracy: 0.4808 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 6.3587 - accuracy: 0.5409 - val_loss: 5.5049 - val_accuracy: 0.4904 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 6.0557 - accuracy: 0.5817 - val_loss: 5.4855 - val_accuracy: 0.4808 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.9887 - accuracy: 0.5865 - val_loss: 5.4920 - val_accuracy: 0.4808 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.8484 - accuracy: 0.6298 - val_loss: 5.1465 - val_accuracy: 0.5962 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 5.9027 - accuracy: 0.5769 - val_loss: 4.9896 - val_accuracy: 0.6154 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 5.7296 - accuracy: 0.5986 - val_loss: 4.8436 - val_accuracy: 0.6827 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.3962 - accuracy: 0.7067 - val_loss: 4.7828 - val_accuracy: 0.6827 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.4612 - accuracy: 0.6659 - val_loss: 4.7588 - val_accuracy: 0.7115 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 2s 75ms/step - loss: 5.3406 - accuracy: 0.6947 - val_loss: 4.7194 - val_accuracy: 0.6731 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 5.2385 - accuracy: 0.6923 - val_loss: 4.8013 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 5.1904 - accuracy: 0.7115 - val_loss: 4.7116 - val_accuracy: 0.6731 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 5.1275 - accuracy: 0.7019 - val_loss: 4.5907 - val_accuracy: 0.7115 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 5.0128 - accuracy: 0.7091 - val_loss: 4.5410 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 4.8452 - accuracy: 0.7284 - val_loss: 4.5558 - val_accuracy: 0.6923 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 4.9586 - accuracy: 0.7043 - val_loss: 4.3415 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.7282 - accuracy: 0.7740 - val_loss: 4.2973 - val_accuracy: 0.7500 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.6208 - accuracy: 0.7788 - val_loss: 4.2564 - val_accuracy: 0.7500 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.7906 - accuracy: 0.7404 - val_loss: 4.3235 - val_accuracy: 0.7500 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.5360 - accuracy: 0.7957 - val_loss: 4.3874 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 4.4796 - accuracy: 0.7933 - val_loss: 4.2760 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 4.3811 - accuracy: 0.8101 - val_loss: 4.0703 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 4.3690 - accuracy: 0.7885 - val_loss: 4.0294 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.2357 - accuracy: 0.8173 - val_loss: 4.1253 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 4.1111 - accuracy: 0.8293 - val_loss: 4.1702 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.2396 - accuracy: 0.8269 - val_loss: 3.9652 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.0779 - accuracy: 0.8389 - val_loss: 3.9659 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.1083 - accuracy: 0.8197 - val_loss: 3.9512 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.9185 - accuracy: 0.8726 - val_loss: 3.7488 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.8902 - accuracy: 0.8654 - val_loss: 3.8764 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.8387 - accuracy: 0.8750 - val_loss: 3.7101 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.8447 - accuracy: 0.8486 - val_loss: 3.7668 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 3.8452 - accuracy: 0.8582 - val_loss: 3.8460 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.9216 - accuracy: 0.8534 - val_loss: 3.8770 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.8217 - accuracy: 0.8510 - val_loss: 3.8580 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 3.7934 - accuracy: 0.8341 - val_loss: 3.7002 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 2s 73ms/step - loss: 3.6401 - accuracy: 0.8678 - val_loss: 3.7017 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.7620 - accuracy: 0.8534 - val_loss: 3.9406 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.6464 - accuracy: 0.8798 - val_loss: 3.5077 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.4894 - accuracy: 0.8990 - val_loss: 3.4562 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.4080 - accuracy: 0.9111 - val_loss: 3.2799 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.4192 - accuracy: 0.8846 - val_loss: 3.6367 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.3340 - accuracy: 0.9087 - val_loss: 3.1868 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.3853 - accuracy: 0.8822 - val_loss: 3.5206 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.2430 - accuracy: 0.9111 - val_loss: 3.1566 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.2425 - accuracy: 0.9111 - val_loss: 3.0622 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.2772 - accuracy: 0.9111 - val_loss: 3.1141 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1483 - accuracy: 0.9087 - val_loss: 3.0238 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.0396 - accuracy: 0.9351 - val_loss: 2.9885 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.1549 - accuracy: 0.9135 - val_loss: 3.0094 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.0912 - accuracy: 0.9279 - val_loss: 3.4754 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.1102 - accuracy: 0.9087 - val_loss: 3.0389 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.0472 - accuracy: 0.9087 - val_loss: 2.8470 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.1080 - accuracy: 0.8798 - val_loss: 2.8873 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.9920 - accuracy: 0.9327 - val_loss: 3.0638 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.9177 - accuracy: 0.9375 - val_loss: 3.0958 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.8531 - accuracy: 0.9327 - val_loss: 2.8067 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.7606 - accuracy: 0.9519 - val_loss: 2.7465 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.7771 - accuracy: 0.9231 - val_loss: 2.7247 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.7010 - accuracy: 0.9447 - val_loss: 2.7353 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.6320 - accuracy: 0.9567 - val_loss: 2.7262 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.6152 - accuracy: 0.9567 - val_loss: 2.6988 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.6463 - accuracy: 0.9447 - val_loss: 2.6411 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 2s 75ms/step - loss: 2.7930 - accuracy: 0.9014 - val_loss: 2.6028 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.6468 - accuracy: 0.9303 - val_loss: 2.9342 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.5959 - accuracy: 0.9495 - val_loss: 2.6095 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.7180 - accuracy: 0.8990 - val_loss: 2.8941 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.5184 - accuracy: 0.9351 - val_loss: 2.6067 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.4810 - accuracy: 0.9495\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.4810 - accuracy: 0.9495 - val_loss: 2.6422 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.3668 - accuracy: 0.9615 - val_loss: 2.6110 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.3779 - accuracy: 0.9615 - val_loss: 2.4707 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.3673 - accuracy: 0.9519 - val_loss: 2.4671 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.3791 - accuracy: 0.9615 - val_loss: 2.5314 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.3922 - accuracy: 0.9423 - val_loss: 2.4906 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 2.3261 - accuracy: 0.9591 - val_loss: 2.4386 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.2904 - accuracy: 0.9712 - val_loss: 2.4414 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.2913 - accuracy: 0.9663 - val_loss: 2.4538 - val_accuracy: 0.8846 - lr: 1.5000e-04\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.2742 - accuracy: 0.9567 - val_loss: 2.4052 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.2224 - accuracy: 0.9760 - val_loss: 2.3349 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.2010 - accuracy: 0.9688 - val_loss: 2.3190 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.2110 - accuracy: 0.9663 - val_loss: 2.3338 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.2392 - accuracy: 0.9688 - val_loss: 2.4596 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.2703 - accuracy: 0.9543 - val_loss: 2.2852 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.2285 - accuracy: 0.9471 - val_loss: 2.2572 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.1583 - accuracy: 0.9663 - val_loss: 2.2839 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.1272 - accuracy: 0.9760 - val_loss: 2.3081 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 2.1466 - accuracy: 0.9663 - val_loss: 2.2844 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.1460 - accuracy: 0.9736 - val_loss: 2.3016 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.0835 - accuracy: 0.9784 - val_loss: 2.2059 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.0869 - accuracy: 0.9784 - val_loss: 2.2871 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.0572 - accuracy: 0.9784 - val_loss: 2.1913 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 2s 74ms/step - loss: 2.1488 - accuracy: 0.9663 - val_loss: 2.1533 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.0669 - accuracy: 0.9808 - val_loss: 2.2033 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.0097 - accuracy: 0.9784 - val_loss: 2.2339 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.0059 - accuracy: 0.9808 - val_loss: 2.2635 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "4/4 [==============================] - 1s 17ms/step\n",
      "\n",
      "Training Fold 2/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 24 samples\n",
      "hammer curl: 7 samples\n",
      "lat pulldown: 19 samples\n",
      "lateral raise: 15 samples\n",
      "pull Up: 10 samples\n",
      "push-up: 22 samples\n",
      "shoulder press: 7 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_1 (Conv1D)           (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 45, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPoolin  (None, 22, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 22, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 64)                256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 7s 87ms/step - loss: 8.3346 - accuracy: 0.2428 - val_loss: 6.4823 - val_accuracy: 0.0769 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 7.5378 - accuracy: 0.3100\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 7.5105 - accuracy: 0.3149 - val_loss: 6.4739 - val_accuracy: 0.0865 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 6.9698 - accuracy: 0.4275\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 6.9577 - accuracy: 0.4303 - val_loss: 6.4005 - val_accuracy: 0.1058 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 6.7362 - accuracy: 0.4425\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 6.7110 - accuracy: 0.4495 - val_loss: 6.2920 - val_accuracy: 0.1731 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 6.5148 - accuracy: 0.5000\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 6.5138 - accuracy: 0.5000 - val_loss: 6.0979 - val_accuracy: 0.2500 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 6.4047 - accuracy: 0.5100\n",
      "Warning: Possible overfitting detected!\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.4131 - accuracy: 0.5048 - val_loss: 5.9006 - val_accuracy: 0.2981 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 6.3699 - accuracy: 0.4736 - val_loss: 5.7580 - val_accuracy: 0.3750 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 5.9918 - accuracy: 0.5673 - val_loss: 5.4968 - val_accuracy: 0.4327 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 6.0230 - accuracy: 0.5673 - val_loss: 5.3777 - val_accuracy: 0.5192 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 5.9595 - accuracy: 0.5721 - val_loss: 5.1292 - val_accuracy: 0.6442 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 5.8166 - accuracy: 0.5433 - val_loss: 4.9790 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 5.5856 - accuracy: 0.6274 - val_loss: 4.8614 - val_accuracy: 0.6827 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 5.5468 - accuracy: 0.6274 - val_loss: 4.7869 - val_accuracy: 0.6731 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 5.5012 - accuracy: 0.6106 - val_loss: 4.6321 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 2s 77ms/step - loss: 5.2899 - accuracy: 0.6611 - val_loss: 4.7180 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 5.2881 - accuracy: 0.6683 - val_loss: 4.5181 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 5.1577 - accuracy: 0.7067 - val_loss: 4.6796 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 5.1637 - accuracy: 0.6683 - val_loss: 4.5010 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 4.9435 - accuracy: 0.7091 - val_loss: 4.3492 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 4.8762 - accuracy: 0.7404 - val_loss: 4.3863 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 4.8440 - accuracy: 0.7236 - val_loss: 4.3221 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 4.7026 - accuracy: 0.7596 - val_loss: 4.4598 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 4.6731 - accuracy: 0.7212 - val_loss: 4.3048 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 1s 51ms/step - loss: 4.4487 - accuracy: 0.7788 - val_loss: 4.2647 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 1s 53ms/step - loss: 4.4793 - accuracy: 0.7812 - val_loss: 4.0750 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.4565 - accuracy: 0.7861 - val_loss: 4.0420 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 4.3618 - accuracy: 0.8197 - val_loss: 3.9832 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.2385 - accuracy: 0.8293 - val_loss: 3.9726 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 4.2955 - accuracy: 0.8005 - val_loss: 4.0074 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 4.1897 - accuracy: 0.8173 - val_loss: 3.7865 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.0573 - accuracy: 0.8558 - val_loss: 3.8279 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 4.0981 - accuracy: 0.8221 - val_loss: 3.8120 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.8727 - accuracy: 0.8486 - val_loss: 3.7840 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.9371 - accuracy: 0.8221 - val_loss: 3.6547 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.7334 - accuracy: 0.8774 - val_loss: 3.4754 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.8082 - accuracy: 0.8630 - val_loss: 3.5498 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.6805 - accuracy: 0.8726 - val_loss: 3.7076 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 3.8781 - accuracy: 0.8413 - val_loss: 3.5480 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.7423 - accuracy: 0.8486 - val_loss: 3.6908 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 3.6759 - accuracy: 0.8575\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.6930 - accuracy: 0.8510 - val_loss: 3.6103 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 3.5436 - accuracy: 0.8846 - val_loss: 3.4830 - val_accuracy: 0.8462 - lr: 1.5000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 1s 30ms/step - loss: 3.4326 - accuracy: 0.9062 - val_loss: 3.3302 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.4584 - accuracy: 0.8966 - val_loss: 3.3211 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.4594 - accuracy: 0.9038 - val_loss: 3.6518 - val_accuracy: 0.7981 - lr: 1.5000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.4263 - accuracy: 0.8894 - val_loss: 3.3485 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.3336 - accuracy: 0.8894 - val_loss: 3.2995 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.2226 - accuracy: 0.9279 - val_loss: 3.2065 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.2683 - accuracy: 0.9399 - val_loss: 3.2419 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.3660 - accuracy: 0.8966 - val_loss: 3.1870 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.2216 - accuracy: 0.9351 - val_loss: 3.1965 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1884 - accuracy: 0.9183 - val_loss: 3.1337 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.1524 - accuracy: 0.9351 - val_loss: 3.0872 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1637 - accuracy: 0.9183 - val_loss: 3.1859 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1018 - accuracy: 0.9423 - val_loss: 3.1201 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.0092 - accuracy: 0.9543 - val_loss: 3.0629 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.0182 - accuracy: 0.9495 - val_loss: 3.0465 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 3.0500 - accuracy: 0.9423 - val_loss: 3.0183 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1126 - accuracy: 0.9207 - val_loss: 3.0391 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.9815 - accuracy: 0.9447 - val_loss: 2.9629 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.9291 - accuracy: 0.9471 - val_loss: 2.9413 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.9674 - accuracy: 0.9351 - val_loss: 2.8928 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.9773 - accuracy: 0.9327 - val_loss: 2.9731 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.9505 - accuracy: 0.9375 - val_loss: 2.9461 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 3.0184 - accuracy: 0.9207 - val_loss: 3.0365 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.8857 - accuracy: 0.9375 - val_loss: 2.9696 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.8598 - accuracy: 0.9519\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 2.8598 - accuracy: 0.9519 - val_loss: 2.9161 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 2.8673 - accuracy: 0.9471 - val_loss: 2.8257 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 2.8484 - accuracy: 0.9375 - val_loss: 2.8175 - val_accuracy: 0.9327 - lr: 7.5000e-05\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.7753 - accuracy: 0.9519 - val_loss: 2.8393 - val_accuracy: 0.9327 - lr: 7.5000e-05\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.8010 - accuracy: 0.9471 - val_loss: 2.8435 - val_accuracy: 0.9327 - lr: 7.5000e-05\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 1s 33ms/step - loss: 2.9480 - accuracy: 0.9159 - val_loss: 2.9067 - val_accuracy: 0.8942 - lr: 7.5000e-05\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.7910 - accuracy: 0.9351 - val_loss: 2.7858 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.7657 - accuracy: 0.9519 - val_loss: 2.7678 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.6604 - accuracy: 0.9760 - val_loss: 2.7545 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.6475 - accuracy: 0.9688 - val_loss: 2.7620 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.6603 - accuracy: 0.9688 - val_loss: 2.7232 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.7137 - accuracy: 0.9567 - val_loss: 2.7080 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.6647 - accuracy: 0.9519 - val_loss: 2.7193 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.6319 - accuracy: 0.9784 - val_loss: 2.6934 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.6564 - accuracy: 0.9567 - val_loss: 2.7393 - val_accuracy: 0.9231 - lr: 7.5000e-05\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.5767 - accuracy: 0.9736 - val_loss: 2.6973 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.6497 - accuracy: 0.9591 - val_loss: 2.6936 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.6831 - accuracy: 0.9543 - val_loss: 2.6495 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.6020 - accuracy: 0.9712 - val_loss: 2.6414 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.5727 - accuracy: 0.9688 - val_loss: 2.6287 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.6213 - accuracy: 0.9639 - val_loss: 2.6019 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.5333 - accuracy: 0.9712 - val_loss: 2.5969 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.4927 - accuracy: 0.9808 - val_loss: 2.6110 - val_accuracy: 0.9327 - lr: 7.5000e-05\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.6029 - accuracy: 0.9471 - val_loss: 2.5829 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.5049 - accuracy: 0.9712 - val_loss: 2.6243 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.5710 - accuracy: 0.9519 - val_loss: 2.6191 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.4793 - accuracy: 0.9663 - val_loss: 2.5976 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 2s 78ms/step - loss: 2.5314 - accuracy: 0.9736 - val_loss: 2.5607 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 2.4575 - accuracy: 0.9712 - val_loss: 2.5455 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 31ms/step - loss: 2.4916 - accuracy: 0.9567 - val_loss: 2.5812 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 1s 29ms/step - loss: 2.4555 - accuracy: 0.9639 - val_loss: 2.5279 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.4946 - accuracy: 0.9567 - val_loss: 2.5894 - val_accuracy: 0.9135 - lr: 7.5000e-05\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.4181 - accuracy: 0.9712 - val_loss: 2.5512 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.4250 - accuracy: 0.9736 - val_loss: 2.5749 - val_accuracy: 0.9231 - lr: 7.5000e-05\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.3990 - accuracy: 0.9760 - val_loss: 2.5736 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "4/4 [==============================] - 1s 13ms/step\n",
      "\n",
      "Training Fold 3/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 24 samples\n",
      "hammer curl: 7 samples\n",
      "lat pulldown: 19 samples\n",
      "lateral raise: 14 samples\n",
      "pull Up: 11 samples\n",
      "push-up: 22 samples\n",
      "shoulder press: 7 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_2 (Conv1D)           (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 45, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPoolin  (None, 22, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 22, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 7s 95ms/step - loss: 8.7960 - accuracy: 0.1971 - val_loss: 6.4090 - val_accuracy: 0.1538 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 7.6432 - accuracy: 0.2740 - val_loss: 6.2918 - val_accuracy: 0.2212 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 7.3534 - accuracy: 0.3630 - val_loss: 6.1760 - val_accuracy: 0.2885 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 6.9901 - accuracy: 0.4111 - val_loss: 6.0016 - val_accuracy: 0.3462 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 6.5611 - accuracy: 0.4712 - val_loss: 5.7693 - val_accuracy: 0.4423 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 6.5311 - accuracy: 0.4784 - val_loss: 5.5333 - val_accuracy: 0.5865 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.1685 - accuracy: 0.5625 - val_loss: 5.2739 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 6.2554 - accuracy: 0.4976 - val_loss: 5.1593 - val_accuracy: 0.6442 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 6.0309 - accuracy: 0.5361 - val_loss: 5.0942 - val_accuracy: 0.5865 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 2s 81ms/step - loss: 5.9784 - accuracy: 0.5505 - val_loss: 5.0451 - val_accuracy: 0.6250 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.8070 - accuracy: 0.6010 - val_loss: 4.9296 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.8853 - accuracy: 0.5625 - val_loss: 4.8128 - val_accuracy: 0.6250 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 5.6727 - accuracy: 0.5913 - val_loss: 4.7538 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.5461 - accuracy: 0.6226 - val_loss: 4.6308 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 5.5368 - accuracy: 0.6130 - val_loss: 4.5310 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 5.4213 - accuracy: 0.6322 - val_loss: 4.5228 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.3151 - accuracy: 0.6322 - val_loss: 4.6015 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 5.3418 - accuracy: 0.6490 - val_loss: 4.5492 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 1s 49ms/step - loss: 5.1203 - accuracy: 0.6827 - val_loss: 4.5012 - val_accuracy: 0.6923 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.1932 - accuracy: 0.6707 - val_loss: 4.4280 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.1563 - accuracy: 0.6659 - val_loss: 4.2780 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 4.8925 - accuracy: 0.6971 - val_loss: 4.2324 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.8175 - accuracy: 0.7260 - val_loss: 4.1449 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 4.9999 - accuracy: 0.6923 - val_loss: 4.1639 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.7511 - accuracy: 0.7236 - val_loss: 4.1382 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.6798 - accuracy: 0.7428 - val_loss: 4.0966 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 4.6641 - accuracy: 0.7380 - val_loss: 4.1518 - val_accuracy: 0.6731 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.6411 - accuracy: 0.7428 - val_loss: 3.8795 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 4.5455 - accuracy: 0.7524 - val_loss: 3.8413 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 4.5463 - accuracy: 0.7476 - val_loss: 4.0882 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.4724 - accuracy: 0.7620 - val_loss: 3.7876 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.3497 - accuracy: 0.7812 - val_loss: 4.0919 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.2705 - accuracy: 0.7909 - val_loss: 3.9023 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 2s 80ms/step - loss: 4.2162 - accuracy: 0.7981 - val_loss: 3.6064 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.1082 - accuracy: 0.8053 - val_loss: 3.6085 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.1224 - accuracy: 0.8029 - val_loss: 3.7133 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.1729 - accuracy: 0.7548 - val_loss: 3.8992 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.9698 - accuracy: 0.8077 - val_loss: 3.4558 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 3.9644 - accuracy: 0.8053 - val_loss: 3.4657 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.8852 - accuracy: 0.8221 - val_loss: 3.4359 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.8370 - accuracy: 0.8269 - val_loss: 3.3509 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.7583 - accuracy: 0.8317 - val_loss: 3.5690 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 3.7002 - accuracy: 0.8486 - val_loss: 3.5227 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.7674 - accuracy: 0.8462 - val_loss: 3.2847 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.7254 - accuracy: 0.8486 - val_loss: 3.2105 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 3.6447 - accuracy: 0.8630 - val_loss: 3.3622 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 3.5524 - accuracy: 0.8606 - val_loss: 3.2506 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.5185 - accuracy: 0.8678 - val_loss: 3.1383 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.5622 - accuracy: 0.8630 - val_loss: 3.1082 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 3.5040 - accuracy: 0.8606 - val_loss: 3.1441 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.4375 - accuracy: 0.8486 - val_loss: 3.2810 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.3050 - accuracy: 0.8678 - val_loss: 3.0742 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.3056 - accuracy: 0.8870 - val_loss: 2.9321 - val_accuracy: 0.9615 - lr: 3.0000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.3189 - accuracy: 0.8678 - val_loss: 3.1040 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 3.1201 - accuracy: 0.9087 - val_loss: 2.9575 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.2531 - accuracy: 0.8678 - val_loss: 3.0429 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.0686 - accuracy: 0.9014 - val_loss: 2.8105 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 2s 85ms/step - loss: 3.1401 - accuracy: 0.8894 - val_loss: 2.7974 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1504 - accuracy: 0.8726 - val_loss: 2.8433 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.1154 - accuracy: 0.8990 - val_loss: 2.8157 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.0285 - accuracy: 0.9062 - val_loss: 3.2258 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.0288 - accuracy: 0.8822 - val_loss: 2.8972 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 63/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 2.9473 - accuracy: 0.9100\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.9479 - accuracy: 0.9087 - val_loss: 2.9221 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.8817 - accuracy: 0.9183 - val_loss: 2.6808 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.8784 - accuracy: 0.9207 - val_loss: 2.6454 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.8599 - accuracy: 0.9111 - val_loss: 2.5707 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.8498 - accuracy: 0.9159 - val_loss: 2.5802 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.7297 - accuracy: 0.9207 - val_loss: 2.5577 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.7266 - accuracy: 0.9351 - val_loss: 2.5185 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.8310 - accuracy: 0.9231 - val_loss: 2.5925 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.6783 - accuracy: 0.9519 - val_loss: 2.5407 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 2.7715 - accuracy: 0.9255 - val_loss: 2.6944 - val_accuracy: 0.8750 - lr: 1.5000e-04\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.7205 - accuracy: 0.9375 - val_loss: 2.4812 - val_accuracy: 0.9808 - lr: 1.5000e-04\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.6118 - accuracy: 0.9399 - val_loss: 2.5120 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.6415 - accuracy: 0.9471 - val_loss: 2.5315 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.8736 - accuracy: 0.9087 - val_loss: 2.5304 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.6778 - accuracy: 0.9423 - val_loss: 2.4656 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.6417 - accuracy: 0.9447 - val_loss: 2.4082 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.5870 - accuracy: 0.9327 - val_loss: 2.4080 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.5463 - accuracy: 0.9495 - val_loss: 2.4521 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.4634 - accuracy: 0.9639 - val_loss: 2.4000 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 1s 52ms/step - loss: 2.5599 - accuracy: 0.9375 - val_loss: 2.4436 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 2.6114 - accuracy: 0.9351 - val_loss: 2.4061 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.4821 - accuracy: 0.9519 - val_loss: 2.3771 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.4282 - accuracy: 0.9712 - val_loss: 2.5440 - val_accuracy: 0.8750 - lr: 1.5000e-04\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.5850 - accuracy: 0.9351 - val_loss: 2.2967 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.4272 - accuracy: 0.9471 - val_loss: 2.2572 - val_accuracy: 0.9904 - lr: 1.5000e-04\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.3929 - accuracy: 0.9495 - val_loss: 2.2796 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.3985 - accuracy: 0.9591 - val_loss: 2.3045 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.3480 - accuracy: 0.9688 - val_loss: 2.2603 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.3719 - accuracy: 0.9688 - val_loss: 2.2931 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.3763 - accuracy: 0.9543 - val_loss: 2.1857 - val_accuracy: 0.9808 - lr: 1.5000e-04\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.4373 - accuracy: 0.9495 - val_loss: 2.3813 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.3295 - accuracy: 0.9615 - val_loss: 2.1684 - val_accuracy: 0.9808 - lr: 1.5000e-04\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.2692 - accuracy: 0.9712 - val_loss: 2.2296 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.2792 - accuracy: 0.9639 - val_loss: 2.2032 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.3246 - accuracy: 0.9591 - val_loss: 2.2230 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.2995 - accuracy: 0.9495 - val_loss: 2.1540 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.2323 - accuracy: 0.9784 - val_loss: 2.1579 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.2838 - accuracy: 0.9615 - val_loss: 2.0976 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "4/4 [==============================] - 1s 15ms/step\n",
      "\n",
      "Training Fold 4/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 23 samples\n",
      "hammer curl: 8 samples\n",
      "lat pulldown: 18 samples\n",
      "lateral raise: 15 samples\n",
      "pull Up: 11 samples\n",
      "push-up: 22 samples\n",
      "shoulder press: 7 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_3 (Conv1D)           (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 45, 64)            256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPoolin  (None, 22, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 22, 128)           512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 8s 96ms/step - loss: 8.3881 - accuracy: 0.2115 - val_loss: 6.3557 - val_accuracy: 0.2788 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 7.3998 - accuracy: 0.4087 - val_loss: 6.2439 - val_accuracy: 0.4135 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 7.0557 - accuracy: 0.4519 - val_loss: 6.1066 - val_accuracy: 0.4423 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 1s 34ms/step - loss: 6.7345 - accuracy: 0.4615 - val_loss: 5.9146 - val_accuracy: 0.5481 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 1s 47ms/step - loss: 6.4341 - accuracy: 0.5264 - val_loss: 5.7229 - val_accuracy: 0.5962 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 6.2349 - accuracy: 0.5673 - val_loss: 5.5257 - val_accuracy: 0.6250 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 6.1420 - accuracy: 0.5841 - val_loss: 5.3770 - val_accuracy: 0.6154 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 5.9957 - accuracy: 0.6058 - val_loss: 5.2542 - val_accuracy: 0.6058 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 6.0472 - accuracy: 0.5841 - val_loss: 5.0627 - val_accuracy: 0.6635 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.8969 - accuracy: 0.5625 - val_loss: 4.9008 - val_accuracy: 0.6923 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 5.5558 - accuracy: 0.6490 - val_loss: 4.7932 - val_accuracy: 0.6635 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.5543 - accuracy: 0.6851 - val_loss: 4.7159 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.3950 - accuracy: 0.6803 - val_loss: 4.6323 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.2950 - accuracy: 0.7139 - val_loss: 4.6146 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 5.0876 - accuracy: 0.7284 - val_loss: 4.5158 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 5.0227 - accuracy: 0.7188 - val_loss: 4.4594 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 5.0756 - accuracy: 0.6947 - val_loss: 4.3992 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 5.0189 - accuracy: 0.6779 - val_loss: 4.4143 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.8592 - accuracy: 0.7572 - val_loss: 4.2228 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 4.7761 - accuracy: 0.7284 - val_loss: 4.3618 - val_accuracy: 0.7115 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 4.7194 - accuracy: 0.7572 - val_loss: 4.1801 - val_accuracy: 0.7115 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.6566 - accuracy: 0.7620 - val_loss: 4.1614 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 4.4965 - accuracy: 0.7861 - val_loss: 3.9468 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 4.5546 - accuracy: 0.7764 - val_loss: 3.9906 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 4.3854 - accuracy: 0.7933 - val_loss: 4.1811 - val_accuracy: 0.6923 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 4.3853 - accuracy: 0.7933 - val_loss: 4.1309 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 4.2047 - accuracy: 0.8269 - val_loss: 4.1283 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 4.2911 - accuracy: 0.7957 - val_loss: 3.9191 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 32ms/step - loss: 4.2304 - accuracy: 0.8077 - val_loss: 3.9197 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 4.2235 - accuracy: 0.7981 - val_loss: 3.9222 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 4.1482 - accuracy: 0.8053 - val_loss: 3.8246 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 4.2505 - accuracy: 0.7740 - val_loss: 3.8051 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.9977 - accuracy: 0.8053 - val_loss: 4.0004 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.9128 - accuracy: 0.8221 - val_loss: 3.6767 - val_accuracy: 0.7500 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 4.0724 - accuracy: 0.7837 - val_loss: 3.6177 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.8207 - accuracy: 0.8438 - val_loss: 3.3769 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.7124 - accuracy: 0.8654 - val_loss: 3.4019 - val_accuracy: 0.8365 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.7443 - accuracy: 0.8486 - val_loss: 3.6399 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.5945 - accuracy: 0.8534 - val_loss: 3.5452 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 3.5867 - accuracy: 0.8726 - val_loss: 3.4655 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 3.6416 - accuracy: 0.8606 - val_loss: 3.2517 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 1s 48ms/step - loss: 3.5076 - accuracy: 0.8750 - val_loss: 3.3508 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.5767 - accuracy: 0.8606 - val_loss: 3.2005 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 3.3650 - accuracy: 0.8870 - val_loss: 3.1730 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.3674 - accuracy: 0.8846 - val_loss: 3.0990 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 46ms/step - loss: 3.2222 - accuracy: 0.9014 - val_loss: 3.0636 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 3.3578 - accuracy: 0.8510 - val_loss: 3.1561 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 3.1621 - accuracy: 0.9183 - val_loss: 2.9586 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 3.2555 - accuracy: 0.8918 - val_loss: 3.1639 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 2s 82ms/step - loss: 3.2159 - accuracy: 0.8966 - val_loss: 2.9341 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.0846 - accuracy: 0.9087 - val_loss: 2.9757 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 3.1680 - accuracy: 0.8990 - val_loss: 2.9347 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.1203 - accuracy: 0.8870 - val_loss: 3.1183 - val_accuracy: 0.8462 - lr: 3.0000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.0816 - accuracy: 0.8918 - val_loss: 2.8741 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 3.0258 - accuracy: 0.8918 - val_loss: 2.8711 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 3.0027 - accuracy: 0.8990 - val_loss: 3.1049 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.9048 - accuracy: 0.9231 - val_loss: 3.0994 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.8509 - accuracy: 0.9062 - val_loss: 2.6930 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.7668 - accuracy: 0.9327 - val_loss: 2.9736 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.8996 - accuracy: 0.9159 - val_loss: 3.0401 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 3.0447 - accuracy: 0.8654 - val_loss: 2.6349 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.7469 - accuracy: 0.9231 - val_loss: 2.5498 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.8008 - accuracy: 0.8966 - val_loss: 2.4758 - val_accuracy: 0.9423 - lr: 3.0000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.7127 - accuracy: 0.9207 - val_loss: 2.4542 - val_accuracy: 0.9423 - lr: 3.0000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.6972 - accuracy: 0.9062 - val_loss: 2.4953 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.5930 - accuracy: 0.9159 - val_loss: 2.4933 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.6094 - accuracy: 0.9327 - val_loss: 2.5730 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.6188 - accuracy: 0.9111 - val_loss: 2.4342 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.5009 - accuracy: 0.9375 - val_loss: 2.3349 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.5284 - accuracy: 0.9351 - val_loss: 2.4067 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.6216 - accuracy: 0.9183 - val_loss: 2.5203 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.4235 - accuracy: 0.9279 - val_loss: 2.3858 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.4253 - accuracy: 0.9327 - val_loss: 2.4748 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 74/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 2.4399 - accuracy: 0.9225\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 2.4391 - accuracy: 0.9231 - val_loss: 2.5201 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 2.3575 - accuracy: 0.9375 - val_loss: 2.2708 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 2s 56ms/step - loss: 2.2746 - accuracy: 0.9543 - val_loss: 2.2068 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 36ms/step - loss: 2.3503 - accuracy: 0.9327 - val_loss: 2.1339 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.3335 - accuracy: 0.9471 - val_loss: 2.1527 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 1s 35ms/step - loss: 2.3991 - accuracy: 0.9279 - val_loss: 2.3028 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.4379 - accuracy: 0.9207 - val_loss: 2.3210 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.3012 - accuracy: 0.9207 - val_loss: 2.3014 - val_accuracy: 0.8942 - lr: 1.5000e-04\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.3106 - accuracy: 0.9375 - val_loss: 2.1256 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.2897 - accuracy: 0.9423 - val_loss: 2.1741 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.2221 - accuracy: 0.9495 - val_loss: 2.0989 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.1945 - accuracy: 0.9591 - val_loss: 2.2334 - val_accuracy: 0.9135 - lr: 1.5000e-04\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.1860 - accuracy: 0.9519 - val_loss: 2.1280 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 2.0881 - accuracy: 0.9760 - val_loss: 2.1147 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.0852 - accuracy: 0.9712 - val_loss: 2.0531 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 1s 37ms/step - loss: 2.1501 - accuracy: 0.9615 - val_loss: 2.1114 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 2.0634 - accuracy: 0.9688 - val_loss: 2.1019 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 1s 45ms/step - loss: 2.0587 - accuracy: 0.9688 - val_loss: 2.0277 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 1s 41ms/step - loss: 2.0964 - accuracy: 0.9591 - val_loss: 2.0312 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.0195 - accuracy: 0.9760 - val_loss: 2.0318 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 2.0095 - accuracy: 0.9688 - val_loss: 2.0490 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 44ms/step - loss: 2.0939 - accuracy: 0.9375 - val_loss: 2.0643 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 96/100\n",
      "25/26 [===========================>..] - ETA: 0s - loss: 1.9976 - accuracy: 0.9550\n",
      "Epoch 96: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "26/26 [==============================] - 1s 39ms/step - loss: 1.9936 - accuracy: 0.9567 - val_loss: 2.0879 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 1s 38ms/step - loss: 2.0211 - accuracy: 0.9567 - val_loss: 2.0425 - val_accuracy: 0.9231 - lr: 7.5000e-05\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 1s 43ms/step - loss: 1.9682 - accuracy: 0.9663 - val_loss: 1.9650 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 1s 42ms/step - loss: 1.9830 - accuracy: 0.9639 - val_loss: 1.9552 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 1s 40ms/step - loss: 1.8981 - accuracy: 0.9856 - val_loss: 1.9398 - val_accuracy: 0.9615 - lr: 7.5000e-05\n",
      "4/4 [==============================] - 2s 17ms/step\n",
      "\n",
      "Training Fold 5/5\n",
      "\n",
      "Class distribution in validation set:\n",
      "barbell biceps curl: 23 samples\n",
      "hammer curl: 8 samples\n",
      "lat pulldown: 18 samples\n",
      "lateral raise: 15 samples\n",
      "pull Up: 10 samples\n",
      "push-up: 23 samples\n",
      "shoulder press: 7 samples\n",
      "\n",
      "Model Summary:\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_4 (Conv1D)           (None, 45, 64)            57088     \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 45, 64)            256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPoolin  (None, 22, 64)            0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 22, 64)            0         \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               (None, 22, 128)           98816     \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 22, 128)           512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 22, 128)           0         \n",
      "                                                                 \n",
      " lstm_9 (LSTM)               (None, 64)                49408     \n",
      "                                                                 \n",
      " batch_normalization_18 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization_19 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 7)                 455       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 211207 (825.03 KB)\n",
      "Trainable params: 210567 (822.53 KB)\n",
      "Non-trainable params: 640 (2.50 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "26/26 [==============================] - 8s 103ms/step - loss: 8.2276 - accuracy: 0.2692 - val_loss: 6.4020 - val_accuracy: 0.1827 - lr: 3.0000e-04\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 7.6124 - accuracy: 0.3870 - val_loss: 6.3455 - val_accuracy: 0.2404 - lr: 3.0000e-04\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 2s 72ms/step - loss: 7.1872 - accuracy: 0.3678 - val_loss: 6.2257 - val_accuracy: 0.2885 - lr: 3.0000e-04\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 6.9014 - accuracy: 0.4615 - val_loss: 6.1054 - val_accuracy: 0.3077 - lr: 3.0000e-04\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 6.5610 - accuracy: 0.4904 - val_loss: 5.8927 - val_accuracy: 0.3846 - lr: 3.0000e-04\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 6.5610 - accuracy: 0.5144 - val_loss: 5.7603 - val_accuracy: 0.4231 - lr: 3.0000e-04\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 6.4113 - accuracy: 0.5553 - val_loss: 5.4997 - val_accuracy: 0.4904 - lr: 3.0000e-04\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 2s 74ms/step - loss: 6.0164 - accuracy: 0.5913 - val_loss: 5.2499 - val_accuracy: 0.5962 - lr: 3.0000e-04\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 5.8103 - accuracy: 0.5913 - val_loss: 5.1494 - val_accuracy: 0.6250 - lr: 3.0000e-04\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 2s 69ms/step - loss: 5.7697 - accuracy: 0.5913 - val_loss: 5.0929 - val_accuracy: 0.5481 - lr: 3.0000e-04\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 3s 103ms/step - loss: 5.7347 - accuracy: 0.6058 - val_loss: 4.8247 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 5.5697 - accuracy: 0.6130 - val_loss: 4.6947 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 2s 60ms/step - loss: 5.5006 - accuracy: 0.6442 - val_loss: 4.7143 - val_accuracy: 0.6827 - lr: 3.0000e-04\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 5.2184 - accuracy: 0.6995 - val_loss: 4.5071 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 5.1809 - accuracy: 0.6947 - val_loss: 4.6031 - val_accuracy: 0.7212 - lr: 3.0000e-04\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 5.0721 - accuracy: 0.7043 - val_loss: 4.5004 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 4.9932 - accuracy: 0.7139 - val_loss: 4.6308 - val_accuracy: 0.6538 - lr: 3.0000e-04\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 4.9941 - accuracy: 0.7212 - val_loss: 4.4630 - val_accuracy: 0.7019 - lr: 3.0000e-04\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 4.8575 - accuracy: 0.7139 - val_loss: 4.2956 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 4.8687 - accuracy: 0.6899 - val_loss: 4.4181 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 4.8252 - accuracy: 0.7043 - val_loss: 4.2152 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 4.6576 - accuracy: 0.7260 - val_loss: 4.2154 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 4.4949 - accuracy: 0.7716 - val_loss: 4.1442 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 4.5054 - accuracy: 0.7909 - val_loss: 4.0996 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 4.3869 - accuracy: 0.7668 - val_loss: 4.0213 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 4.3467 - accuracy: 0.7933 - val_loss: 4.1677 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 3s 101ms/step - loss: 4.3275 - accuracy: 0.7837 - val_loss: 3.9784 - val_accuracy: 0.7788 - lr: 3.0000e-04\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 4.2198 - accuracy: 0.8101 - val_loss: 3.9118 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 4.1637 - accuracy: 0.8101 - val_loss: 3.8801 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 4.1325 - accuracy: 0.8077 - val_loss: 3.7791 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 4.0441 - accuracy: 0.8269 - val_loss: 3.6851 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 4.0011 - accuracy: 0.8197 - val_loss: 3.6433 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 3.9577 - accuracy: 0.8413 - val_loss: 3.9131 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 3.8478 - accuracy: 0.8462 - val_loss: 3.8994 - val_accuracy: 0.7308 - lr: 3.0000e-04\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 3.7659 - accuracy: 0.8510 - val_loss: 3.7948 - val_accuracy: 0.7404 - lr: 3.0000e-04\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.6632 - accuracy: 0.8534 - val_loss: 3.5244 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.6789 - accuracy: 0.8678 - val_loss: 3.6018 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.7629 - accuracy: 0.8317 - val_loss: 3.5280 - val_accuracy: 0.7981 - lr: 3.0000e-04\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 3.7293 - accuracy: 0.8389 - val_loss: 3.4729 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.5328 - accuracy: 0.8702 - val_loss: 3.3426 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 3.6543 - accuracy: 0.8438 - val_loss: 3.3334 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.8079 - accuracy: 0.8149 - val_loss: 3.2619 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 2s 98ms/step - loss: 3.6209 - accuracy: 0.8462 - val_loss: 3.2497 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.3885 - accuracy: 0.8798 - val_loss: 3.2599 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 3.3462 - accuracy: 0.8798 - val_loss: 3.1708 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 1s 54ms/step - loss: 3.2741 - accuracy: 0.8702 - val_loss: 3.1061 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 3.3211 - accuracy: 0.8558 - val_loss: 3.0011 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 3.1248 - accuracy: 0.9207 - val_loss: 3.1536 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.2363 - accuracy: 0.8822 - val_loss: 2.9042 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.2479 - accuracy: 0.8798 - val_loss: 2.9685 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 3.1565 - accuracy: 0.8798 - val_loss: 2.8532 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 3.1292 - accuracy: 0.8870 - val_loss: 3.0793 - val_accuracy: 0.8077 - lr: 3.0000e-04\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 3.1024 - accuracy: 0.8870 - val_loss: 3.0475 - val_accuracy: 0.7885 - lr: 3.0000e-04\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 3.0104 - accuracy: 0.8798 - val_loss: 2.8918 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 3.0575 - accuracy: 0.8966 - val_loss: 3.1440 - val_accuracy: 0.8269 - lr: 3.0000e-04\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 2.9799 - accuracy: 0.8966 - val_loss: 2.8311 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.9646 - accuracy: 0.9014 - val_loss: 2.7248 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 2s 61ms/step - loss: 2.8386 - accuracy: 0.9087 - val_loss: 2.6785 - val_accuracy: 0.9135 - lr: 3.0000e-04\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 2s 69ms/step - loss: 2.8129 - accuracy: 0.9279 - val_loss: 2.6944 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 2s 82ms/step - loss: 2.8927 - accuracy: 0.8918 - val_loss: 2.6239 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 2s 58ms/step - loss: 2.7027 - accuracy: 0.9087 - val_loss: 2.7748 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 1s 55ms/step - loss: 2.6844 - accuracy: 0.9111 - val_loss: 2.6422 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 2.7688 - accuracy: 0.8822 - val_loss: 2.5718 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 2s 68ms/step - loss: 2.6576 - accuracy: 0.9062 - val_loss: 2.5735 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 2.5847 - accuracy: 0.9399 - val_loss: 3.1485 - val_accuracy: 0.7596 - lr: 3.0000e-04\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.7638 - accuracy: 0.8918 - val_loss: 2.7043 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.6141 - accuracy: 0.9207 - val_loss: 2.7068 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.6693 - accuracy: 0.9207 - val_loss: 2.5095 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 2s 66ms/step - loss: 2.6179 - accuracy: 0.9303 - val_loss: 2.5805 - val_accuracy: 0.8750 - lr: 3.0000e-04\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 2.5530 - accuracy: 0.9038 - val_loss: 2.4441 - val_accuracy: 0.9038 - lr: 3.0000e-04\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 2.5236 - accuracy: 0.9231 - val_loss: 2.2946 - val_accuracy: 0.9519 - lr: 3.0000e-04\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.4263 - accuracy: 0.9159 - val_loss: 2.2184 - val_accuracy: 0.9615 - lr: 3.0000e-04\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.3906 - accuracy: 0.9279 - val_loss: 2.2285 - val_accuracy: 0.9615 - lr: 3.0000e-04\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.5317 - accuracy: 0.9159 - val_loss: 2.4557 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 2s 94ms/step - loss: 2.2917 - accuracy: 0.9255 - val_loss: 2.4104 - val_accuracy: 0.8654 - lr: 3.0000e-04\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 2.3678 - accuracy: 0.9279 - val_loss: 2.2321 - val_accuracy: 0.9231 - lr: 3.0000e-04\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 2.2688 - accuracy: 0.9399 - val_loss: 2.1968 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 2.2221 - accuracy: 0.9399 - val_loss: 2.5147 - val_accuracy: 0.8173 - lr: 3.0000e-04\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 2s 59ms/step - loss: 2.2164 - accuracy: 0.9423 - val_loss: 2.0469 - val_accuracy: 0.9519 - lr: 3.0000e-04\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 2.2728 - accuracy: 0.9447 - val_loss: 2.2368 - val_accuracy: 0.8558 - lr: 3.0000e-04\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 2.3521 - accuracy: 0.8918 - val_loss: 2.3777 - val_accuracy: 0.8846 - lr: 3.0000e-04\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.2986 - accuracy: 0.8990 - val_loss: 2.6526 - val_accuracy: 0.7692 - lr: 3.0000e-04\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 2.2637 - accuracy: 0.9183 - val_loss: 2.2760 - val_accuracy: 0.8942 - lr: 3.0000e-04\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 2.2350 - accuracy: 0.9159\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 0.0001500000071246177.\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 2.2350 - accuracy: 0.9159 - val_loss: 2.0716 - val_accuracy: 0.9327 - lr: 3.0000e-04\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 2s 70ms/step - loss: 2.1895 - accuracy: 0.9231 - val_loss: 2.0354 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 2s 63ms/step - loss: 2.1369 - accuracy: 0.9375 - val_loss: 1.9325 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.0475 - accuracy: 0.9591 - val_loss: 1.9004 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 2.0951 - accuracy: 0.9519 - val_loss: 1.9328 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.0034 - accuracy: 0.9543 - val_loss: 1.9423 - val_accuracy: 0.9423 - lr: 1.5000e-04\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 2.1037 - accuracy: 0.9351 - val_loss: 2.0345 - val_accuracy: 0.9327 - lr: 1.5000e-04\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 1.9841 - accuracy: 0.9495 - val_loss: 1.8736 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 3s 88ms/step - loss: 1.9610 - accuracy: 0.9591 - val_loss: 1.8527 - val_accuracy: 0.9712 - lr: 1.5000e-04\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 1s 57ms/step - loss: 1.9310 - accuracy: 0.9760 - val_loss: 2.1650 - val_accuracy: 0.9038 - lr: 1.5000e-04\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 1s 56ms/step - loss: 1.9049 - accuracy: 0.9688 - val_loss: 2.0286 - val_accuracy: 0.9231 - lr: 1.5000e-04\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 1s 58ms/step - loss: 1.9408 - accuracy: 0.9639 - val_loss: 1.8798 - val_accuracy: 0.9519 - lr: 1.5000e-04\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 2s 62ms/step - loss: 1.8710 - accuracy: 0.9808 - val_loss: 1.8657 - val_accuracy: 0.9615 - lr: 1.5000e-04\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - ETA: 0s - loss: 1.9696 - accuracy: 0.9615\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 7.500000356230885e-05.\n",
      "26/26 [==============================] - 2s 65ms/step - loss: 1.9696 - accuracy: 0.9615 - val_loss: 2.4466 - val_accuracy: 0.8558 - lr: 1.5000e-04\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 2.0110 - accuracy: 0.9423 - val_loss: 2.0644 - val_accuracy: 0.8942 - lr: 7.5000e-05\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 2s 67ms/step - loss: 1.8427 - accuracy: 0.9784 - val_loss: 1.9017 - val_accuracy: 0.9423 - lr: 7.5000e-05\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 2s 64ms/step - loss: 1.9836 - accuracy: 0.9471 - val_loss: 1.8902 - val_accuracy: 0.9519 - lr: 7.5000e-05\n",
      "4/4 [==============================] - 1s 21ms/step\n",
      "\n",
      "K-Fold Cross Validation Results (Enhanced CNN-LSTM with Weighted Features):\n",
      "Mean Validation Accuracy: 0.9442 ± 0.0185\n",
      "Mean Validation Loss: 2.1529 ± 0.2769\n",
      "Average Epochs per Fold: 100.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Updated paths to match new data location\n",
    "    data_dir = r\"D:\\minor\\workout_processed_data_landmark\"\n",
    "    save_path = r\"D:\\minor\\k_fold_CNN_LSTM_landmark\"\n",
    "    \n",
    "    trainer = WorkoutModelTrainer(data_dir)\n",
    "    fold_results, fold_histories = trainer.train_with_kfold(\n",
    "        epochs=100,\n",
    "        batch_size=16,\n",
    "        n_splits=5,\n",
    "        save_path=save_path\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1:\n",
      "Reloading Tuner from tuner_results\\fold_1\\tuner0.json\n",
      "Best hyperparameters: {'filters': 32, 'l2_lambda': 0.1, 'dropout_rate': 0.3, 'lstm_units': 256, 'learning_rate': 0.001}\n",
      "\n",
      "Fold 2:\n",
      "Reloading Tuner from tuner_results\\fold_2\\tuner0.json\n",
      "Best hyperparameters: {'filters': 128, 'l2_lambda': 0.01, 'dropout_rate': 0.4, 'lstm_units': 128, 'learning_rate': 0.0003}\n",
      "\n",
      "Fold 3:\n",
      "Reloading Tuner from tuner_results\\fold_3\\tuner0.json\n",
      "Best hyperparameters: {'filters': 64, 'l2_lambda': 0.01, 'dropout_rate': 0.35, 'lstm_units': 256, 'learning_rate': 0.001}\n",
      "\n",
      "Fold 4:\n",
      "Reloading Tuner from tuner_results\\fold_4\\tuner0.json\n",
      "Best hyperparameters: {'filters': 64, 'l2_lambda': 0.01, 'dropout_rate': 0.35, 'lstm_units': 128, 'learning_rate': 0.001}\n",
      "\n",
      "Fold 5:\n",
      "Reloading Tuner from tuner_results\\fold_5\\tuner0.json\n",
      "Best hyperparameters: {'filters': 128, 'l2_lambda': 0.01, 'dropout_rate': 0.4, 'lstm_units': 256, 'learning_rate': 0.0003}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Load data\n",
    "data_dir = r\"D:\\minor\\workout_processed_data_landmark\"\n",
    "X = np.load(os.path.join(data_dir, 'X.npy'))\n",
    "y = np.load(os.path.join(data_dir, 'y.npy'))\n",
    "confidences = np.load(os.path.join(data_dir, 'confidences.npy'))\n",
    "class_mapping = np.load(os.path.join(data_dir, 'class_mapping.npy'), allow_pickle=True).item()\n",
    "\n",
    "# Define hyperparameter tuning model\n",
    "def model_builder(hp):\n",
    "    model = tf.keras.Sequential([\n",
    "        Conv1D(\n",
    "            filters=hp.Choice('filters', [32, 64, 128]), \n",
    "            kernel_size=3, \n",
    "            activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_lambda', [0.001, 0.01, 0.1]))\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(hp.Choice('dropout_rate', [0.3, 0.35, 0.4])),\n",
    "\n",
    "        LSTM(hp.Choice('lstm_units', [64, 128, 256]), return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        Dropout(hp.Choice('dropout_rate', [0.3, 0.35, 0.4])),\n",
    "\n",
    "        LSTM(64),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', [0.0001, 0.0003, 0.001])),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# K-Fold Cross Validation (5 folds)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "    print(f\"\\nFold {fold+1}:\")\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Define Keras Tuner\n",
    "    tuner = kt.RandomSearch(\n",
    "        model_builder, \n",
    "        objective='val_accuracy', \n",
    "        max_trials=10,\n",
    "        directory=\"tuner_results\",\n",
    "        project_name=f\"fold_{fold+1}\"\n",
    "    )\n",
    "\n",
    "    # Run hyperparameter tuning\n",
    "    tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Get best hyperparameters\n",
    "    best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "    print(\"Best hyperparameters:\", best_hyperparameters.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, num_classes, lstm_units_1=128, lstm_units_2=64):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(64, kernel_size=3, padding='same', activation='relu',\n",
    "                              kernel_regularizer=tf.keras.regularizers.l2(0.01)), \n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Dropout(0.35),\n",
    "    \n",
    "        tf.keras.layers.LSTM(lstm_units_1, return_sequences=True,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.35),\n",
    "    \n",
    "        tf.keras.layers.LSTM(lstm_units_2, kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.35),\n",
    "    \n",
    "        tf.keras.layers.Dense(64, activation='relu',\n",
    "                             kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "    \n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "# Example: Define X and y (replace with your actual data)\n",
    "X = np.load(os.path.join(data_dir, 'X.npy'))\n",
    "y = np.load(os.path.join(data_dir, 'y.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 10ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 8ms/step\n",
      "4/4 [==============================] - 2s 10ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 11ms/step\n",
      "4/4 [==============================] - 2s 15ms/step\n",
      "4/4 [==============================] - 0s 19ms/step\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      "4/4 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define a range of LSTM units to test\n",
    "lstm_units_list = [32, 64, 128]\n",
    "bias_list = []\n",
    "variance_list = []\n",
    "\n",
    "# Use stratified k-fold cross-validation to estimate bias and variance\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for lstm_units in lstm_units_list:\n",
    "    # Build the model with the current LSTM units\n",
    "    model = build_model(input_shape=(45, 297), num_classes=7, lstm_units_1=lstm_units, lstm_units_2=lstm_units // 2)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Store predictions for each fold\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X, y):  # y contains integer class labels\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "        \n",
    "        # Predict on the validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        predictions.append(y_pred)\n",
    "        true_labels.append(y_val)\n",
    "    \n",
    "    # Calculate bias and variance\n",
    "    predictions = np.array(predictions)  # Shape: (num_folds, num_samples, num_classes)\n",
    "    true_labels = np.array(true_labels)  # Shape: (num_folds, num_samples)\n",
    "    \n",
    "    # Bias: Mean squared error between predicted probabilities and true labels\n",
    "    # Convert true labels to one-hot encoding for comparison\n",
    "    true_labels_one_hot = tf.keras.utils.to_categorical(true_labels, num_classes=7)\n",
    "    bias = np.mean((np.mean(predictions, axis=0) - true_labels_one_hot) ** 2)\n",
    "    \n",
    "    # Variance: Variance of predicted probabilities across folds\n",
    "    variance = np.mean(np.var(predictions, axis=0))\n",
    "    \n",
    "    bias_list.append(bias)\n",
    "    variance_list.append(variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate total error (bias + variance)\n",
    "total_error = [b + v for b, v in zip(bias_list, variance_list)]\n",
    "\n",
    "# Plot the curves\n",
    "plt.plot(lstm_units_list, bias_list, label='Bias', marker='o')\n",
    "plt.plot(lstm_units_list, variance_list, label='Variance', marker='o')\n",
    "plt.plot(lstm_units_list, total_error, label='Total Error', marker='o')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Model Complexity (LSTM Units)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Bias-Variance Trade-Off Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
